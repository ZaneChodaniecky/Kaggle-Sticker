{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "87ecc9be-8eb3-42cf-a103-f882ac54b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import socket\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import holidays\n",
    "import pycountry\n",
    "import optuna\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from termcolor import colored\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3d21237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available with 1 GPU(s).\n",
      "CatBoost can use the GPU.\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from catboost.utils import get_gpu_device_count\n",
    "\n",
    "# Check available GPUs\n",
    "gpu_count = get_gpu_device_count()\n",
    "if gpu_count > 0:\n",
    "    print(f\"GPU is available with {gpu_count} GPU(s).\")\n",
    "else:\n",
    "    print(\"No GPU detected. Ensure your setup is correct.\")\n",
    "\n",
    "# Test CatBoost with GPU\n",
    "try:\n",
    "    model = CatBoostClassifier(task_type=\"GPU\", devices='0')  # Specify GPU\n",
    "    device = 'gpu'\n",
    "    print(\"CatBoost can use the GPU.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    device = 'cpu'\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2ee7c11a-7155-4d25-a8de-24e6c05d0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare important variables\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "submission_path = 'sample_submission.csv'\n",
    "target_variable = 'num_sold'\n",
    "SEED = 69\n",
    "skip_hypertuning = True\n",
    "cat_encoder_type = 'OneHotEncoder' #OneHotEncoder, FrequencyEncoder, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2a6afd61-d48c-4d0a-8272-39ad990662f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframes(path1: str, path2: str, format1: str = \"csv\", format2: str = \"csv\") -> tuple:\n",
    "\n",
    "    loaders = {\n",
    "        \"csv\": pd.read_csv,\n",
    "        \"excel\": pd.read_excel,\n",
    "        \"json\": pd.read_json,\n",
    "    }\n",
    "\n",
    "    if format1 not in loaders or format2 not in loaders:\n",
    "        raise ValueError(\"Unsupported format. Supported formats: 'csv', 'excel', 'json'.\")\n",
    "\n",
    "    # Load the dataframes using appropriate loaders\n",
    "    df1 = loaders[format1](path1)\n",
    "    df2 = loaders[format2](path2)\n",
    "\n",
    "    print(f\"Loading data from {os.getcwd()}\")\n",
    "    \n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0fce9fbd-384a-4fac-8bbf-3eb68d30d172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from C:\\Users\\ZaneC\\OneDrive\\Documents\\Python\\Kaggle\\Playground Series\\s5e1 - Sticker Sales\n"
     ]
    }
   ],
   "source": [
    "train, test = load_dataframes(train_path,test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "60d6e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'dataset' column to distinguish train and test data\n",
    "train['dataset'] = 'train'\n",
    "test['dataset'] = 'test'\n",
    "\n",
    "# concatenate the datasets with the added 'dataset' column\n",
    "df = pd.concat([train, test], axis=0).reset_index(drop=True)\n",
    "df = df.rename(columns={target_variable: 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3d828194-d954-4d0e-b48a-020dcd4d6ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_kde_grid(df_train,n_cols=4,figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "37f42554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_kde_grid(df_solve,n_cols=4,figsize=(9,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "81cb01aa-4360-417b-9fc7-4070a5384573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_cols(df: pd.DataFrame):\n",
    "    num_cols = df.select_dtypes(include=['number']).columns.to_list()\n",
    "    \n",
    "    return num_cols\n",
    "\n",
    "def get_cat_cols(df: pd.DataFrame):\n",
    "    cat_cols = df.select_dtypes(include=['object', 'string', 'category']).columns.tolist()  \n",
    "\n",
    "    return cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f68956-e3b5-45ab-a4d1-5be3e5a18fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7a363157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_dataframe(df_to_use: pd.DataFrame, name: str = 'DataFrame', nrows: int = 3, plots: bool = False, info: bool = True) -> None:\n",
    "    '''\n",
    "    Function to describe the DataFrame with summary statistics, missing value count,\n",
    "    unique value count, and duplicate count. It also displays plots for missing and unique values.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame to describe.\n",
    "    - name: The name to display in the summary (default is 'DataFrame').\n",
    "    - nrows: The number of rows to display from the top and bottom (default is 3).\n",
    "    - plots: Whether to display bar plots for missing and unique values (default is False).\n",
    "    - info: Whether to display the styled DataFrame (default is True).\n",
    "    '''\n",
    "\n",
    "    df = df_to_use.copy()   \n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    inf = pd.DataFrame(df.dtypes).reset_index().rename(columns={'index':'column', 0:'type'})\n",
    "    \n",
    "    # Missing values\n",
    "    df_missing = pd.DataFrame(df.isnull().sum()).reset_index().rename(columns={'index':'column', 0:'missing'})\n",
    "    df_missing['pct_missing'] = (df_missing['missing'] / df.shape[0]) * 100\n",
    "    \n",
    "    # Unique values\n",
    "    df_unique = pd.DataFrame(df.nunique()).reset_index().rename(columns={'index':'column', 0:'unique'})\n",
    "    \n",
    "    # Combine summary information\n",
    "    inf['missing'] = df_missing['missing']\n",
    "    inf['pct_missing'] = df_missing['pct_missing']\n",
    "    inf['unique'] = df_unique['unique']\n",
    "    inf['duplicate'] = df.duplicated().sum()\n",
    "    inf['count'] = df.shape[0]\n",
    "\n",
    "    # Descriptive statistics\n",
    "    desc = pd.DataFrame(df.describe(include='all').transpose())\n",
    "    if 'min' in desc.columns.tolist():\n",
    "        inf['min'] = desc['min'].values\n",
    "        inf['max'] = desc['max'].values\n",
    "        inf['avg'] = desc['mean'].values\n",
    "        inf['std dev'] = desc['std'].values\n",
    "    if 'top' in desc.columns.tolist():\n",
    "        inf['top value'] = desc['top'].values\n",
    "        inf['Freq'] = desc['freq'].values \n",
    "    \n",
    "    # Display styled DataFrame\n",
    "    if info:\n",
    "        display(inf.style.background_gradient(subset=['missing','pct_missing'], cmap='Reds').background_gradient(subset='unique', cmap='Greens'))\n",
    "\n",
    "    if nrows != 0 :\n",
    "        # Display top and bottom nrows of the DataFrame\n",
    "        print(f\"\\n---------- {name} Overview ----------:\")\n",
    "        print(f\"{name} has {df.shape[0]} rows and {df.shape[1]} columns\\n\")\n",
    "        display(df.head(nrows))\n",
    "        display(df.tail(nrows))\n",
    "    \n",
    "    # Plot missing values if any\n",
    "    if plots and df_missing['missing'].sum() > 0:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(24, 5))\n",
    "        sns.barplot(df_missing[df_missing['missing'] > 0], x='column', y='missing', ax=ax)\n",
    "        ax.set_title(f'{name} missing Values') \n",
    "        ax.bar_label(ax.containers[0])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "        # Plot unique values\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(24, 5))\n",
    "        sns.barplot(df_unique[df_unique['unique'] > 0], x='column', y='unique', ax=ax)\n",
    "        ax.set_title(f'{name} Unique Values')\n",
    "        ax.bar_label(ax.containers[0])\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "da182033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_47be9_row0_col2, #T_47be9_row0_col3, #T_47be9_row1_col2, #T_47be9_row1_col3, #T_47be9_row2_col2, #T_47be9_row2_col3, #T_47be9_row3_col2, #T_47be9_row3_col3, #T_47be9_row4_col2, #T_47be9_row4_col3, #T_47be9_row6_col2, #T_47be9_row6_col3 {\n",
       "  background-color: #fff5f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_47be9_row0_col4 {\n",
       "  background-color: #00441b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_47be9_row1_col4 {\n",
       "  background-color: #f6fcf4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_47be9_row2_col4, #T_47be9_row3_col4, #T_47be9_row4_col4, #T_47be9_row6_col4 {\n",
       "  background-color: #f7fcf5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_47be9_row5_col2, #T_47be9_row5_col3 {\n",
       "  background-color: #67000d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_47be9_row5_col4 {\n",
       "  background-color: #f5fbf3;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_47be9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_47be9_level0_col0\" class=\"col_heading level0 col0\" >column</th>\n",
       "      <th id=\"T_47be9_level0_col1\" class=\"col_heading level0 col1\" >type</th>\n",
       "      <th id=\"T_47be9_level0_col2\" class=\"col_heading level0 col2\" >missing</th>\n",
       "      <th id=\"T_47be9_level0_col3\" class=\"col_heading level0 col3\" >pct_missing</th>\n",
       "      <th id=\"T_47be9_level0_col4\" class=\"col_heading level0 col4\" >unique</th>\n",
       "      <th id=\"T_47be9_level0_col5\" class=\"col_heading level0 col5\" >duplicate</th>\n",
       "      <th id=\"T_47be9_level0_col6\" class=\"col_heading level0 col6\" >count</th>\n",
       "      <th id=\"T_47be9_level0_col7\" class=\"col_heading level0 col7\" >min</th>\n",
       "      <th id=\"T_47be9_level0_col8\" class=\"col_heading level0 col8\" >max</th>\n",
       "      <th id=\"T_47be9_level0_col9\" class=\"col_heading level0 col9\" >avg</th>\n",
       "      <th id=\"T_47be9_level0_col10\" class=\"col_heading level0 col10\" >std dev</th>\n",
       "      <th id=\"T_47be9_level0_col11\" class=\"col_heading level0 col11\" >top value</th>\n",
       "      <th id=\"T_47be9_level0_col12\" class=\"col_heading level0 col12\" >Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_47be9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_47be9_row0_col0\" class=\"data row0 col0\" >id</td>\n",
       "      <td id=\"T_47be9_row0_col1\" class=\"data row0 col1\" >int64</td>\n",
       "      <td id=\"T_47be9_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_47be9_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "      <td id=\"T_47be9_row0_col4\" class=\"data row0 col4\" >328680</td>\n",
       "      <td id=\"T_47be9_row0_col5\" class=\"data row0 col5\" >0</td>\n",
       "      <td id=\"T_47be9_row0_col6\" class=\"data row0 col6\" >328680</td>\n",
       "      <td id=\"T_47be9_row0_col7\" class=\"data row0 col7\" >0.000000</td>\n",
       "      <td id=\"T_47be9_row0_col8\" class=\"data row0 col8\" >328679.000000</td>\n",
       "      <td id=\"T_47be9_row0_col9\" class=\"data row0 col9\" >164339.500000</td>\n",
       "      <td id=\"T_47be9_row0_col10\" class=\"data row0 col10\" >94881.887576</td>\n",
       "      <td id=\"T_47be9_row0_col11\" class=\"data row0 col11\" >nan</td>\n",
       "      <td id=\"T_47be9_row0_col12\" class=\"data row0 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47be9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_47be9_row1_col0\" class=\"data row1 col0\" >date</td>\n",
       "      <td id=\"T_47be9_row1_col1\" class=\"data row1 col1\" >object</td>\n",
       "      <td id=\"T_47be9_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_47be9_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "      <td id=\"T_47be9_row1_col4\" class=\"data row1 col4\" >3652</td>\n",
       "      <td id=\"T_47be9_row1_col5\" class=\"data row1 col5\" >0</td>\n",
       "      <td id=\"T_47be9_row1_col6\" class=\"data row1 col6\" >328680</td>\n",
       "      <td id=\"T_47be9_row1_col7\" class=\"data row1 col7\" >nan</td>\n",
       "      <td id=\"T_47be9_row1_col8\" class=\"data row1 col8\" >nan</td>\n",
       "      <td id=\"T_47be9_row1_col9\" class=\"data row1 col9\" >nan</td>\n",
       "      <td id=\"T_47be9_row1_col10\" class=\"data row1 col10\" >nan</td>\n",
       "      <td id=\"T_47be9_row1_col11\" class=\"data row1 col11\" >2010-01-01</td>\n",
       "      <td id=\"T_47be9_row1_col12\" class=\"data row1 col12\" >90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47be9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_47be9_row2_col0\" class=\"data row2 col0\" >country</td>\n",
       "      <td id=\"T_47be9_row2_col1\" class=\"data row2 col1\" >object</td>\n",
       "      <td id=\"T_47be9_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_47be9_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
       "      <td id=\"T_47be9_row2_col4\" class=\"data row2 col4\" >6</td>\n",
       "      <td id=\"T_47be9_row2_col5\" class=\"data row2 col5\" >0</td>\n",
       "      <td id=\"T_47be9_row2_col6\" class=\"data row2 col6\" >328680</td>\n",
       "      <td id=\"T_47be9_row2_col7\" class=\"data row2 col7\" >nan</td>\n",
       "      <td id=\"T_47be9_row2_col8\" class=\"data row2 col8\" >nan</td>\n",
       "      <td id=\"T_47be9_row2_col9\" class=\"data row2 col9\" >nan</td>\n",
       "      <td id=\"T_47be9_row2_col10\" class=\"data row2 col10\" >nan</td>\n",
       "      <td id=\"T_47be9_row2_col11\" class=\"data row2 col11\" >Canada</td>\n",
       "      <td id=\"T_47be9_row2_col12\" class=\"data row2 col12\" >54780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47be9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_47be9_row3_col0\" class=\"data row3 col0\" >store</td>\n",
       "      <td id=\"T_47be9_row3_col1\" class=\"data row3 col1\" >object</td>\n",
       "      <td id=\"T_47be9_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_47be9_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "      <td id=\"T_47be9_row3_col4\" class=\"data row3 col4\" >3</td>\n",
       "      <td id=\"T_47be9_row3_col5\" class=\"data row3 col5\" >0</td>\n",
       "      <td id=\"T_47be9_row3_col6\" class=\"data row3 col6\" >328680</td>\n",
       "      <td id=\"T_47be9_row3_col7\" class=\"data row3 col7\" >nan</td>\n",
       "      <td id=\"T_47be9_row3_col8\" class=\"data row3 col8\" >nan</td>\n",
       "      <td id=\"T_47be9_row3_col9\" class=\"data row3 col9\" >nan</td>\n",
       "      <td id=\"T_47be9_row3_col10\" class=\"data row3 col10\" >nan</td>\n",
       "      <td id=\"T_47be9_row3_col11\" class=\"data row3 col11\" >Discount Stickers</td>\n",
       "      <td id=\"T_47be9_row3_col12\" class=\"data row3 col12\" >109560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47be9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_47be9_row4_col0\" class=\"data row4 col0\" >product</td>\n",
       "      <td id=\"T_47be9_row4_col1\" class=\"data row4 col1\" >object</td>\n",
       "      <td id=\"T_47be9_row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "      <td id=\"T_47be9_row4_col3\" class=\"data row4 col3\" >0.000000</td>\n",
       "      <td id=\"T_47be9_row4_col4\" class=\"data row4 col4\" >5</td>\n",
       "      <td id=\"T_47be9_row4_col5\" class=\"data row4 col5\" >0</td>\n",
       "      <td id=\"T_47be9_row4_col6\" class=\"data row4 col6\" >328680</td>\n",
       "      <td id=\"T_47be9_row4_col7\" class=\"data row4 col7\" >nan</td>\n",
       "      <td id=\"T_47be9_row4_col8\" class=\"data row4 col8\" >nan</td>\n",
       "      <td id=\"T_47be9_row4_col9\" class=\"data row4 col9\" >nan</td>\n",
       "      <td id=\"T_47be9_row4_col10\" class=\"data row4 col10\" >nan</td>\n",
       "      <td id=\"T_47be9_row4_col11\" class=\"data row4 col11\" >Holographic Goose</td>\n",
       "      <td id=\"T_47be9_row4_col12\" class=\"data row4 col12\" >65736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47be9_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_47be9_row5_col0\" class=\"data row5 col0\" >y</td>\n",
       "      <td id=\"T_47be9_row5_col1\" class=\"data row5 col1\" >float64</td>\n",
       "      <td id=\"T_47be9_row5_col2\" class=\"data row5 col2\" >107421</td>\n",
       "      <td id=\"T_47be9_row5_col3\" class=\"data row5 col3\" >32.682548</td>\n",
       "      <td id=\"T_47be9_row5_col4\" class=\"data row5 col4\" >4037</td>\n",
       "      <td id=\"T_47be9_row5_col5\" class=\"data row5 col5\" >0</td>\n",
       "      <td id=\"T_47be9_row5_col6\" class=\"data row5 col6\" >328680</td>\n",
       "      <td id=\"T_47be9_row5_col7\" class=\"data row5 col7\" >5.000000</td>\n",
       "      <td id=\"T_47be9_row5_col8\" class=\"data row5 col8\" >5939.000000</td>\n",
       "      <td id=\"T_47be9_row5_col9\" class=\"data row5 col9\" >752.527382</td>\n",
       "      <td id=\"T_47be9_row5_col10\" class=\"data row5 col10\" >690.165445</td>\n",
       "      <td id=\"T_47be9_row5_col11\" class=\"data row5 col11\" >nan</td>\n",
       "      <td id=\"T_47be9_row5_col12\" class=\"data row5 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47be9_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_47be9_row6_col0\" class=\"data row6 col0\" >dataset</td>\n",
       "      <td id=\"T_47be9_row6_col1\" class=\"data row6 col1\" >object</td>\n",
       "      <td id=\"T_47be9_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "      <td id=\"T_47be9_row6_col3\" class=\"data row6 col3\" >0.000000</td>\n",
       "      <td id=\"T_47be9_row6_col4\" class=\"data row6 col4\" >2</td>\n",
       "      <td id=\"T_47be9_row6_col5\" class=\"data row6 col5\" >0</td>\n",
       "      <td id=\"T_47be9_row6_col6\" class=\"data row6 col6\" >328680</td>\n",
       "      <td id=\"T_47be9_row6_col7\" class=\"data row6 col7\" >nan</td>\n",
       "      <td id=\"T_47be9_row6_col8\" class=\"data row6 col8\" >nan</td>\n",
       "      <td id=\"T_47be9_row6_col9\" class=\"data row6 col9\" >nan</td>\n",
       "      <td id=\"T_47be9_row6_col10\" class=\"data row6 col10\" >nan</td>\n",
       "      <td id=\"T_47be9_row6_col11\" class=\"data row6 col11\" >train</td>\n",
       "      <td id=\"T_47be9_row6_col12\" class=\"data row6 col12\" >230130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1d2176cf750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "describe_dataframe(df, name='Insurance Data', nrows=0, plots=False, info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6ae05780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "df.boxplot(column='y', by='country', grid=False)\n",
    "plt.title(\"Outliers in 'num_sold' by Country\")\n",
    "plt.suptitle(\"\")  # Removes the default matplotlib title\n",
    "plt.ylabel('Number of Units Sold')\n",
    "plt.xlabel('Country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ca00c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd004e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "83b6522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "df.boxplot(column='y', by='country', grid=False)\n",
    "plt.title(\"Outliers in 'num_sold' by Country\")\n",
    "plt.suptitle(\"\")  # Removes the default matplotlib title\n",
    "plt.ylabel('Number of Units Sold')\n",
    "plt.xlabel('Country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1e3a9767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Outliers: 6630\n"
     ]
    }
   ],
   "source": [
    "# Calculate Outlier Bounds Using IQR\n",
    "# Calculating Interquartile Range (IQR) to identify potential outliers\n",
    "# Filtering rows where 'num_sold' is outside the lower and upper bounds\n",
    "\n",
    "# Recalculate Q1, Q3, and IQR\n",
    "Q1 = df['y'].quantile(0.25)\n",
    "Q3 = df['y'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define lower and upper bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter outliers\n",
    "outliers = df[(df['y'] < lower_bound) | (df['y'] > upper_bound)]\n",
    "print(f\"Number of Outliers: {len(outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a514938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df_to_use: pd.DataFrame, impute_num_nulls: bool = True, fill_cat_nulls: bool = True) -> None:\n",
    "    df = df_to_use.copy()\n",
    "\n",
    "    # Convert date column from object to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Filter columns where the type is either 'float' or 'int' and there are missing values\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    missing_numeric_columns = [\n",
    "        col for col in numeric_cols if df[col].isnull().sum() > 0\n",
    "    ]\n",
    "\n",
    "    # Target field 'num_sold' is missing some values, we do not want to impute them\n",
    "    if 'y' in missing_numeric_columns:\n",
    "        missing_numeric_columns.remove('y')\n",
    "    \n",
    "    # Fill nulls in numeric columns with the median\n",
    "    if impute_num_nulls:       \n",
    "        for column in missing_numeric_columns:\n",
    "            mdn = df[column].median()\n",
    "            df[column] = df[column].fillna(mdn)\n",
    "\n",
    "    \n",
    "    # Get category columns\n",
    "    if fill_cat_nulls:\n",
    "        cat_cols = df.select_dtypes(include=['object', 'string','category']).columns.tolist()  \n",
    "        # Fill missing values in object columns\n",
    "        for column in cat_cols:\n",
    "            df[column] = df[column].fillna('None')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b213027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataframe(df, impute_num_nulls = True, fill_cat_nulls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8a79de08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7e96d_row0_col2, #T_7e96d_row0_col3, #T_7e96d_row1_col2, #T_7e96d_row1_col3, #T_7e96d_row2_col2, #T_7e96d_row2_col3, #T_7e96d_row3_col2, #T_7e96d_row3_col3, #T_7e96d_row4_col2, #T_7e96d_row4_col3, #T_7e96d_row6_col2, #T_7e96d_row6_col3 {\n",
       "  background-color: #fff5f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7e96d_row0_col4 {\n",
       "  background-color: #00441b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7e96d_row1_col4 {\n",
       "  background-color: #f6fcf4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7e96d_row2_col4, #T_7e96d_row3_col4, #T_7e96d_row4_col4, #T_7e96d_row6_col4 {\n",
       "  background-color: #f7fcf5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7e96d_row5_col2, #T_7e96d_row5_col3 {\n",
       "  background-color: #67000d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7e96d_row5_col4 {\n",
       "  background-color: #f5fbf3;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7e96d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7e96d_level0_col0\" class=\"col_heading level0 col0\" >column</th>\n",
       "      <th id=\"T_7e96d_level0_col1\" class=\"col_heading level0 col1\" >type</th>\n",
       "      <th id=\"T_7e96d_level0_col2\" class=\"col_heading level0 col2\" >missing</th>\n",
       "      <th id=\"T_7e96d_level0_col3\" class=\"col_heading level0 col3\" >pct_missing</th>\n",
       "      <th id=\"T_7e96d_level0_col4\" class=\"col_heading level0 col4\" >unique</th>\n",
       "      <th id=\"T_7e96d_level0_col5\" class=\"col_heading level0 col5\" >duplicate</th>\n",
       "      <th id=\"T_7e96d_level0_col6\" class=\"col_heading level0 col6\" >count</th>\n",
       "      <th id=\"T_7e96d_level0_col7\" class=\"col_heading level0 col7\" >min</th>\n",
       "      <th id=\"T_7e96d_level0_col8\" class=\"col_heading level0 col8\" >max</th>\n",
       "      <th id=\"T_7e96d_level0_col9\" class=\"col_heading level0 col9\" >avg</th>\n",
       "      <th id=\"T_7e96d_level0_col10\" class=\"col_heading level0 col10\" >std dev</th>\n",
       "      <th id=\"T_7e96d_level0_col11\" class=\"col_heading level0 col11\" >top value</th>\n",
       "      <th id=\"T_7e96d_level0_col12\" class=\"col_heading level0 col12\" >Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7e96d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7e96d_row0_col0\" class=\"data row0 col0\" >id</td>\n",
       "      <td id=\"T_7e96d_row0_col1\" class=\"data row0 col1\" >int64</td>\n",
       "      <td id=\"T_7e96d_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_7e96d_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "      <td id=\"T_7e96d_row0_col4\" class=\"data row0 col4\" >328680</td>\n",
       "      <td id=\"T_7e96d_row0_col5\" class=\"data row0 col5\" >0</td>\n",
       "      <td id=\"T_7e96d_row0_col6\" class=\"data row0 col6\" >328680</td>\n",
       "      <td id=\"T_7e96d_row0_col7\" class=\"data row0 col7\" >0.000000</td>\n",
       "      <td id=\"T_7e96d_row0_col8\" class=\"data row0 col8\" >328679.000000</td>\n",
       "      <td id=\"T_7e96d_row0_col9\" class=\"data row0 col9\" >164339.500000</td>\n",
       "      <td id=\"T_7e96d_row0_col10\" class=\"data row0 col10\" >94881.887576</td>\n",
       "      <td id=\"T_7e96d_row0_col11\" class=\"data row0 col11\" >nan</td>\n",
       "      <td id=\"T_7e96d_row0_col12\" class=\"data row0 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7e96d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7e96d_row1_col0\" class=\"data row1 col0\" >date</td>\n",
       "      <td id=\"T_7e96d_row1_col1\" class=\"data row1 col1\" >datetime64[ns]</td>\n",
       "      <td id=\"T_7e96d_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_7e96d_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "      <td id=\"T_7e96d_row1_col4\" class=\"data row1 col4\" >3652</td>\n",
       "      <td id=\"T_7e96d_row1_col5\" class=\"data row1 col5\" >0</td>\n",
       "      <td id=\"T_7e96d_row1_col6\" class=\"data row1 col6\" >328680</td>\n",
       "      <td id=\"T_7e96d_row1_col7\" class=\"data row1 col7\" >2010-01-01 00:00:00</td>\n",
       "      <td id=\"T_7e96d_row1_col8\" class=\"data row1 col8\" >2019-12-31 00:00:00</td>\n",
       "      <td id=\"T_7e96d_row1_col9\" class=\"data row1 col9\" >2014-12-31 12:00:00</td>\n",
       "      <td id=\"T_7e96d_row1_col10\" class=\"data row1 col10\" >nan</td>\n",
       "      <td id=\"T_7e96d_row1_col11\" class=\"data row1 col11\" >nan</td>\n",
       "      <td id=\"T_7e96d_row1_col12\" class=\"data row1 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7e96d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7e96d_row2_col0\" class=\"data row2 col0\" >country</td>\n",
       "      <td id=\"T_7e96d_row2_col1\" class=\"data row2 col1\" >object</td>\n",
       "      <td id=\"T_7e96d_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_7e96d_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
       "      <td id=\"T_7e96d_row2_col4\" class=\"data row2 col4\" >6</td>\n",
       "      <td id=\"T_7e96d_row2_col5\" class=\"data row2 col5\" >0</td>\n",
       "      <td id=\"T_7e96d_row2_col6\" class=\"data row2 col6\" >328680</td>\n",
       "      <td id=\"T_7e96d_row2_col7\" class=\"data row2 col7\" >nan</td>\n",
       "      <td id=\"T_7e96d_row2_col8\" class=\"data row2 col8\" >nan</td>\n",
       "      <td id=\"T_7e96d_row2_col9\" class=\"data row2 col9\" >nan</td>\n",
       "      <td id=\"T_7e96d_row2_col10\" class=\"data row2 col10\" >nan</td>\n",
       "      <td id=\"T_7e96d_row2_col11\" class=\"data row2 col11\" >Canada</td>\n",
       "      <td id=\"T_7e96d_row2_col12\" class=\"data row2 col12\" >54780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7e96d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7e96d_row3_col0\" class=\"data row3 col0\" >store</td>\n",
       "      <td id=\"T_7e96d_row3_col1\" class=\"data row3 col1\" >object</td>\n",
       "      <td id=\"T_7e96d_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_7e96d_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "      <td id=\"T_7e96d_row3_col4\" class=\"data row3 col4\" >3</td>\n",
       "      <td id=\"T_7e96d_row3_col5\" class=\"data row3 col5\" >0</td>\n",
       "      <td id=\"T_7e96d_row3_col6\" class=\"data row3 col6\" >328680</td>\n",
       "      <td id=\"T_7e96d_row3_col7\" class=\"data row3 col7\" >nan</td>\n",
       "      <td id=\"T_7e96d_row3_col8\" class=\"data row3 col8\" >nan</td>\n",
       "      <td id=\"T_7e96d_row3_col9\" class=\"data row3 col9\" >nan</td>\n",
       "      <td id=\"T_7e96d_row3_col10\" class=\"data row3 col10\" >nan</td>\n",
       "      <td id=\"T_7e96d_row3_col11\" class=\"data row3 col11\" >Discount Stickers</td>\n",
       "      <td id=\"T_7e96d_row3_col12\" class=\"data row3 col12\" >109560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7e96d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7e96d_row4_col0\" class=\"data row4 col0\" >product</td>\n",
       "      <td id=\"T_7e96d_row4_col1\" class=\"data row4 col1\" >object</td>\n",
       "      <td id=\"T_7e96d_row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "      <td id=\"T_7e96d_row4_col3\" class=\"data row4 col3\" >0.000000</td>\n",
       "      <td id=\"T_7e96d_row4_col4\" class=\"data row4 col4\" >5</td>\n",
       "      <td id=\"T_7e96d_row4_col5\" class=\"data row4 col5\" >0</td>\n",
       "      <td id=\"T_7e96d_row4_col6\" class=\"data row4 col6\" >328680</td>\n",
       "      <td id=\"T_7e96d_row4_col7\" class=\"data row4 col7\" >nan</td>\n",
       "      <td id=\"T_7e96d_row4_col8\" class=\"data row4 col8\" >nan</td>\n",
       "      <td id=\"T_7e96d_row4_col9\" class=\"data row4 col9\" >nan</td>\n",
       "      <td id=\"T_7e96d_row4_col10\" class=\"data row4 col10\" >nan</td>\n",
       "      <td id=\"T_7e96d_row4_col11\" class=\"data row4 col11\" >Holographic Goose</td>\n",
       "      <td id=\"T_7e96d_row4_col12\" class=\"data row4 col12\" >65736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7e96d_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_7e96d_row5_col0\" class=\"data row5 col0\" >y</td>\n",
       "      <td id=\"T_7e96d_row5_col1\" class=\"data row5 col1\" >float64</td>\n",
       "      <td id=\"T_7e96d_row5_col2\" class=\"data row5 col2\" >107421</td>\n",
       "      <td id=\"T_7e96d_row5_col3\" class=\"data row5 col3\" >32.682548</td>\n",
       "      <td id=\"T_7e96d_row5_col4\" class=\"data row5 col4\" >4037</td>\n",
       "      <td id=\"T_7e96d_row5_col5\" class=\"data row5 col5\" >0</td>\n",
       "      <td id=\"T_7e96d_row5_col6\" class=\"data row5 col6\" >328680</td>\n",
       "      <td id=\"T_7e96d_row5_col7\" class=\"data row5 col7\" >5.000000</td>\n",
       "      <td id=\"T_7e96d_row5_col8\" class=\"data row5 col8\" >5939.000000</td>\n",
       "      <td id=\"T_7e96d_row5_col9\" class=\"data row5 col9\" >752.527382</td>\n",
       "      <td id=\"T_7e96d_row5_col10\" class=\"data row5 col10\" >690.165445</td>\n",
       "      <td id=\"T_7e96d_row5_col11\" class=\"data row5 col11\" >nan</td>\n",
       "      <td id=\"T_7e96d_row5_col12\" class=\"data row5 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7e96d_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_7e96d_row6_col0\" class=\"data row6 col0\" >dataset</td>\n",
       "      <td id=\"T_7e96d_row6_col1\" class=\"data row6 col1\" >object</td>\n",
       "      <td id=\"T_7e96d_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "      <td id=\"T_7e96d_row6_col3\" class=\"data row6 col3\" >0.000000</td>\n",
       "      <td id=\"T_7e96d_row6_col4\" class=\"data row6 col4\" >2</td>\n",
       "      <td id=\"T_7e96d_row6_col5\" class=\"data row6 col5\" >0</td>\n",
       "      <td id=\"T_7e96d_row6_col6\" class=\"data row6 col6\" >328680</td>\n",
       "      <td id=\"T_7e96d_row6_col7\" class=\"data row6 col7\" >nan</td>\n",
       "      <td id=\"T_7e96d_row6_col8\" class=\"data row6 col8\" >nan</td>\n",
       "      <td id=\"T_7e96d_row6_col9\" class=\"data row6 col9\" >nan</td>\n",
       "      <td id=\"T_7e96d_row6_col10\" class=\"data row6 col10\" >nan</td>\n",
       "      <td id=\"T_7e96d_row6_col11\" class=\"data row6 col11\" >train</td>\n",
       "      <td id=\"T_7e96d_row6_col12\" class=\"data row6 col12\" >230130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1d26b5cb590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "describe_dataframe(df, name='Insurance Data', nrows=0, plots=False, info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93f87d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1c23432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_holiday_name(country_code, date_obj):\n",
    "    try:\n",
    "        country_holiday = holidays.CountryHoliday(country_code, years=date_obj.year)\n",
    "        return country_holiday.get(date_obj)\n",
    "    except Exception as e:\n",
    "        print(f\"Error for country code {country_code} and date {date_obj}: {e}\")\n",
    "        return 'Invalid Holiday'\n",
    "    return country_holiday.get(date_obj)\n",
    "\n",
    "def get_country_code(country_name):\n",
    "    try:\n",
    "        country = pycountry.countries.get(name=country_name)\n",
    "        return country.alpha_2  \n",
    "    except KeyError:\n",
    "        print(f\"Unknown Country: {country_name}\")\n",
    "        return None\n",
    "\n",
    "def get_holiday_for_row(row):\n",
    "    country_code = get_country_code(row['country'])\n",
    "    if country_code is None:\n",
    "        return 'Unknown Country'     \n",
    "    try:\n",
    "        date_obj = row['date']\n",
    "    except ValueError:\n",
    "        print(f\"Invalid Date: {row['date']}\")\n",
    "        return 'Invalid Date'\n",
    "\n",
    "    return get_holiday_name(country_code, date_obj)\n",
    "\n",
    "\n",
    "df['holiday'] = df.apply(get_holiday_for_row, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "35f7d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Year'] = df['date'].dt.year\n",
    "df['Quarter'] = df['date'].dt.quarter\n",
    "df['Month'] = df['date'].dt.month\n",
    "df['Day'] = df['date'].dt.day\n",
    "df['day_of_week'] = df['date'].dt.day_name()\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['Day'] / 365.0)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['Day'] / 365.0)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['Month'] / 12.0)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['Month'] / 12.0)\n",
    "df['year_sin'] = np.sin(2 * np.pi * df['Year'] / 7.0)\n",
    "df['year_cos'] = np.cos(2 * np.pi * df['Year'] / 7.0)\n",
    "df['Group']=(df['Year']-2010)*48+df['Month']*4+df['Day']//7\n",
    "\n",
    "\n",
    "df['Quarter'] = df['Quarter'].astype('str')\n",
    "df['Month'] = df['Month'].astype('str')\n",
    "df['day_of_week'] = df['day_of_week'].astype('str')\n",
    "df['day_of_year'] = df['day_of_week'].astype('str')\n",
    "df['week_of_year'] = df['week_of_year'].astype('str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f80b5ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE 18 BASIC FEATURES ARE:\n",
      "country (categorical) with 6 unique values\n",
      "store (categorical) with 3 unique values\n",
      "product (categorical) with 5 unique values\n",
      "holiday (categorical) with 75 unique values\n",
      "Year (numerical) with 10 unique values\n",
      "Quarter (categorical) with 4 unique values\n",
      "Month (categorical) with 12 unique values\n",
      "Day (numerical) with 31 unique values\n",
      "day_of_week (categorical) with 7 unique values\n",
      "day_of_year (categorical) with 7 unique values\n",
      "week_of_year (categorical) with 53 unique values\n",
      "day_sin (numerical) with 31 unique values\n",
      "day_cos (numerical) with 31 unique values\n",
      "month_sin (numerical) with 8 unique values\n",
      "month_cos (numerical) with 8 unique values\n",
      "year_sin (numerical) with 7 unique values\n",
      "year_cos (numerical) with 4 unique values\n",
      "Group (numerical) with 481 unique values\n",
      "\n",
      "THE FOLLOWING HAVE 9 OR MORE UNIQUE VALUES: ['holiday', 'Year', 'Month', 'Day', 'week_of_year', 'day_sin', 'day_cos', 'Group']\n"
     ]
    }
   ],
   "source": [
    "RMV = ['y','dataset','id','date']\n",
    "FEATURES = [c for c in df.columns if not c in RMV]\n",
    "\n",
    "CATS = []\n",
    "HIGH_CARDINALITY = []\n",
    "print(f\"THE {len(FEATURES)} BASIC FEATURES ARE:\")\n",
    "\n",
    "for c in FEATURES:\n",
    "    ftype = \"numerical\"\n",
    "    if df[c].dtype==\"object\":\n",
    "        CATS.append(c)\n",
    "        df[c] = df[c].fillna(\"NAN\")\n",
    "        df[c],_ = df[c].factorize()\n",
    "        df[c] -= df[c].min()\n",
    "        ftype = \"categorical\"\n",
    "    if df[c].dtype==\"int64\":\n",
    "        df[c] = df[c].astype(\"int32\")\n",
    "    elif df[c].dtype==\"float64\":\n",
    "        df[c] = df[c].astype(\"float32\")\n",
    "\n",
    "    n = df[c].nunique()\n",
    "    print(f\"{c} ({ftype}) with {n} unique values\")\n",
    "    if n>=9: HIGH_CARDINALITY.append(c)\n",
    "\n",
    "print(\"\\nTHE FOLLOWING HAVE 9 OR MORE UNIQUE VALUES:\", HIGH_CARDINALITY )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1782e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "57239c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a9035_row0_col2, #T_a9035_row0_col3, #T_a9035_row1_col2, #T_a9035_row1_col3, #T_a9035_row2_col2, #T_a9035_row2_col3, #T_a9035_row3_col2, #T_a9035_row3_col3, #T_a9035_row4_col2, #T_a9035_row4_col3, #T_a9035_row6_col2, #T_a9035_row6_col3, #T_a9035_row7_col2, #T_a9035_row7_col3, #T_a9035_row8_col2, #T_a9035_row8_col3, #T_a9035_row9_col2, #T_a9035_row9_col3, #T_a9035_row10_col2, #T_a9035_row10_col3, #T_a9035_row11_col2, #T_a9035_row11_col3, #T_a9035_row12_col2, #T_a9035_row12_col3, #T_a9035_row13_col2, #T_a9035_row13_col3, #T_a9035_row14_col2, #T_a9035_row14_col3, #T_a9035_row15_col2, #T_a9035_row15_col3, #T_a9035_row16_col2, #T_a9035_row16_col3, #T_a9035_row17_col2, #T_a9035_row17_col3, #T_a9035_row18_col2, #T_a9035_row18_col3, #T_a9035_row19_col2, #T_a9035_row19_col3, #T_a9035_row20_col2, #T_a9035_row20_col3, #T_a9035_row21_col2, #T_a9035_row21_col3 {\n",
       "  background-color: #fff5f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a9035_row0_col4 {\n",
       "  background-color: #00441b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a9035_row1_col4 {\n",
       "  background-color: #f6fcf4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a9035_row2_col4, #T_a9035_row3_col4, #T_a9035_row4_col4, #T_a9035_row6_col4, #T_a9035_row7_col4, #T_a9035_row8_col4, #T_a9035_row9_col4, #T_a9035_row10_col4, #T_a9035_row11_col4, #T_a9035_row12_col4, #T_a9035_row13_col4, #T_a9035_row14_col4, #T_a9035_row15_col4, #T_a9035_row16_col4, #T_a9035_row17_col4, #T_a9035_row18_col4, #T_a9035_row19_col4, #T_a9035_row20_col4, #T_a9035_row21_col4 {\n",
       "  background-color: #f7fcf5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a9035_row5_col2, #T_a9035_row5_col3 {\n",
       "  background-color: #67000d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a9035_row5_col4 {\n",
       "  background-color: #f5fbf3;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a9035\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a9035_level0_col0\" class=\"col_heading level0 col0\" >column</th>\n",
       "      <th id=\"T_a9035_level0_col1\" class=\"col_heading level0 col1\" >type</th>\n",
       "      <th id=\"T_a9035_level0_col2\" class=\"col_heading level0 col2\" >missing</th>\n",
       "      <th id=\"T_a9035_level0_col3\" class=\"col_heading level0 col3\" >pct_missing</th>\n",
       "      <th id=\"T_a9035_level0_col4\" class=\"col_heading level0 col4\" >unique</th>\n",
       "      <th id=\"T_a9035_level0_col5\" class=\"col_heading level0 col5\" >duplicate</th>\n",
       "      <th id=\"T_a9035_level0_col6\" class=\"col_heading level0 col6\" >count</th>\n",
       "      <th id=\"T_a9035_level0_col7\" class=\"col_heading level0 col7\" >min</th>\n",
       "      <th id=\"T_a9035_level0_col8\" class=\"col_heading level0 col8\" >max</th>\n",
       "      <th id=\"T_a9035_level0_col9\" class=\"col_heading level0 col9\" >avg</th>\n",
       "      <th id=\"T_a9035_level0_col10\" class=\"col_heading level0 col10\" >std dev</th>\n",
       "      <th id=\"T_a9035_level0_col11\" class=\"col_heading level0 col11\" >top value</th>\n",
       "      <th id=\"T_a9035_level0_col12\" class=\"col_heading level0 col12\" >Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a9035_row0_col0\" class=\"data row0 col0\" >id</td>\n",
       "      <td id=\"T_a9035_row0_col1\" class=\"data row0 col1\" >int64</td>\n",
       "      <td id=\"T_a9035_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row0_col4\" class=\"data row0 col4\" >328680</td>\n",
       "      <td id=\"T_a9035_row0_col5\" class=\"data row0 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row0_col6\" class=\"data row0 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row0_col7\" class=\"data row0 col7\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row0_col8\" class=\"data row0 col8\" >328679.000000</td>\n",
       "      <td id=\"T_a9035_row0_col9\" class=\"data row0 col9\" >164339.500000</td>\n",
       "      <td id=\"T_a9035_row0_col10\" class=\"data row0 col10\" >94881.887576</td>\n",
       "      <td id=\"T_a9035_row0_col11\" class=\"data row0 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row0_col12\" class=\"data row0 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a9035_row1_col0\" class=\"data row1 col0\" >date</td>\n",
       "      <td id=\"T_a9035_row1_col1\" class=\"data row1 col1\" >datetime64[ns]</td>\n",
       "      <td id=\"T_a9035_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row1_col4\" class=\"data row1 col4\" >3652</td>\n",
       "      <td id=\"T_a9035_row1_col5\" class=\"data row1 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row1_col6\" class=\"data row1 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row1_col7\" class=\"data row1 col7\" >2010-01-01 00:00:00</td>\n",
       "      <td id=\"T_a9035_row1_col8\" class=\"data row1 col8\" >2019-12-31 00:00:00</td>\n",
       "      <td id=\"T_a9035_row1_col9\" class=\"data row1 col9\" >2014-12-31 12:00:00</td>\n",
       "      <td id=\"T_a9035_row1_col10\" class=\"data row1 col10\" >nan</td>\n",
       "      <td id=\"T_a9035_row1_col11\" class=\"data row1 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row1_col12\" class=\"data row1 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a9035_row2_col0\" class=\"data row2 col0\" >country</td>\n",
       "      <td id=\"T_a9035_row2_col1\" class=\"data row2 col1\" >int32</td>\n",
       "      <td id=\"T_a9035_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row2_col4\" class=\"data row2 col4\" >6</td>\n",
       "      <td id=\"T_a9035_row2_col5\" class=\"data row2 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row2_col6\" class=\"data row2 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row2_col8\" class=\"data row2 col8\" >5.000000</td>\n",
       "      <td id=\"T_a9035_row2_col9\" class=\"data row2 col9\" >2.500000</td>\n",
       "      <td id=\"T_a9035_row2_col10\" class=\"data row2 col10\" >1.707828</td>\n",
       "      <td id=\"T_a9035_row2_col11\" class=\"data row2 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row2_col12\" class=\"data row2 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a9035_row3_col0\" class=\"data row3 col0\" >store</td>\n",
       "      <td id=\"T_a9035_row3_col1\" class=\"data row3 col1\" >int32</td>\n",
       "      <td id=\"T_a9035_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row3_col4\" class=\"data row3 col4\" >3</td>\n",
       "      <td id=\"T_a9035_row3_col5\" class=\"data row3 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row3_col6\" class=\"data row3 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row3_col7\" class=\"data row3 col7\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row3_col8\" class=\"data row3 col8\" >2.000000</td>\n",
       "      <td id=\"T_a9035_row3_col9\" class=\"data row3 col9\" >1.000000</td>\n",
       "      <td id=\"T_a9035_row3_col10\" class=\"data row3 col10\" >0.816498</td>\n",
       "      <td id=\"T_a9035_row3_col11\" class=\"data row3 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row3_col12\" class=\"data row3 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_a9035_row4_col0\" class=\"data row4 col0\" >product</td>\n",
       "      <td id=\"T_a9035_row4_col1\" class=\"data row4 col1\" >int32</td>\n",
       "      <td id=\"T_a9035_row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row4_col3\" class=\"data row4 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row4_col4\" class=\"data row4 col4\" >5</td>\n",
       "      <td id=\"T_a9035_row4_col5\" class=\"data row4 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row4_col6\" class=\"data row4 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row4_col7\" class=\"data row4 col7\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row4_col8\" class=\"data row4 col8\" >4.000000</td>\n",
       "      <td id=\"T_a9035_row4_col9\" class=\"data row4 col9\" >2.000000</td>\n",
       "      <td id=\"T_a9035_row4_col10\" class=\"data row4 col10\" >1.414216</td>\n",
       "      <td id=\"T_a9035_row4_col11\" class=\"data row4 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row4_col12\" class=\"data row4 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_a9035_row5_col0\" class=\"data row5 col0\" >y</td>\n",
       "      <td id=\"T_a9035_row5_col1\" class=\"data row5 col1\" >float64</td>\n",
       "      <td id=\"T_a9035_row5_col2\" class=\"data row5 col2\" >107421</td>\n",
       "      <td id=\"T_a9035_row5_col3\" class=\"data row5 col3\" >32.682548</td>\n",
       "      <td id=\"T_a9035_row5_col4\" class=\"data row5 col4\" >4037</td>\n",
       "      <td id=\"T_a9035_row5_col5\" class=\"data row5 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row5_col6\" class=\"data row5 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row5_col7\" class=\"data row5 col7\" >5.000000</td>\n",
       "      <td id=\"T_a9035_row5_col8\" class=\"data row5 col8\" >5939.000000</td>\n",
       "      <td id=\"T_a9035_row5_col9\" class=\"data row5 col9\" >752.527382</td>\n",
       "      <td id=\"T_a9035_row5_col10\" class=\"data row5 col10\" >690.165445</td>\n",
       "      <td id=\"T_a9035_row5_col11\" class=\"data row5 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row5_col12\" class=\"data row5 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_a9035_row6_col0\" class=\"data row6 col0\" >dataset</td>\n",
       "      <td id=\"T_a9035_row6_col1\" class=\"data row6 col1\" >object</td>\n",
       "      <td id=\"T_a9035_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row6_col3\" class=\"data row6 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row6_col4\" class=\"data row6 col4\" >2</td>\n",
       "      <td id=\"T_a9035_row6_col5\" class=\"data row6 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row6_col6\" class=\"data row6 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row6_col7\" class=\"data row6 col7\" >nan</td>\n",
       "      <td id=\"T_a9035_row6_col8\" class=\"data row6 col8\" >nan</td>\n",
       "      <td id=\"T_a9035_row6_col9\" class=\"data row6 col9\" >nan</td>\n",
       "      <td id=\"T_a9035_row6_col10\" class=\"data row6 col10\" >nan</td>\n",
       "      <td id=\"T_a9035_row6_col11\" class=\"data row6 col11\" >train</td>\n",
       "      <td id=\"T_a9035_row6_col12\" class=\"data row6 col12\" >230130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_a9035_row7_col0\" class=\"data row7 col0\" >holiday</td>\n",
       "      <td id=\"T_a9035_row7_col1\" class=\"data row7 col1\" >int32</td>\n",
       "      <td id=\"T_a9035_row7_col2\" class=\"data row7 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row7_col3\" class=\"data row7 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row7_col4\" class=\"data row7 col4\" >75</td>\n",
       "      <td id=\"T_a9035_row7_col5\" class=\"data row7 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row7_col6\" class=\"data row7 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row7_col7\" class=\"data row7 col7\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row7_col8\" class=\"data row7 col8\" >74.000000</td>\n",
       "      <td id=\"T_a9035_row7_col9\" class=\"data row7 col9\" >4.791804</td>\n",
       "      <td id=\"T_a9035_row7_col10\" class=\"data row7 col10\" >5.508516</td>\n",
       "      <td id=\"T_a9035_row7_col11\" class=\"data row7 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row7_col12\" class=\"data row7 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_a9035_row8_col0\" class=\"data row8 col0\" >Year</td>\n",
       "      <td id=\"T_a9035_row8_col1\" class=\"data row8 col1\" >int32</td>\n",
       "      <td id=\"T_a9035_row8_col2\" class=\"data row8 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row8_col3\" class=\"data row8 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row8_col4\" class=\"data row8 col4\" >10</td>\n",
       "      <td id=\"T_a9035_row8_col5\" class=\"data row8 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row8_col6\" class=\"data row8 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row8_col7\" class=\"data row8 col7\" >2010.000000</td>\n",
       "      <td id=\"T_a9035_row8_col8\" class=\"data row8 col8\" >2019.000000</td>\n",
       "      <td id=\"T_a9035_row8_col9\" class=\"data row8 col9\" >2014.499726</td>\n",
       "      <td id=\"T_a9035_row8_col10\" class=\"data row8 col10\" >2.871904</td>\n",
       "      <td id=\"T_a9035_row8_col11\" class=\"data row8 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row8_col12\" class=\"data row8 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_a9035_row9_col0\" class=\"data row9 col0\" >Quarter</td>\n",
       "      <td id=\"T_a9035_row9_col1\" class=\"data row9 col1\" >int32</td>\n",
       "      <td id=\"T_a9035_row9_col2\" class=\"data row9 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row9_col3\" class=\"data row9 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row9_col4\" class=\"data row9 col4\" >4</td>\n",
       "      <td id=\"T_a9035_row9_col5\" class=\"data row9 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row9_col6\" class=\"data row9 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row9_col7\" class=\"data row9 col7\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row9_col8\" class=\"data row9 col8\" >3.000000</td>\n",
       "      <td id=\"T_a9035_row9_col9\" class=\"data row9 col9\" >1.508762</td>\n",
       "      <td id=\"T_a9035_row9_col10\" class=\"data row9 col10\" >1.117021</td>\n",
       "      <td id=\"T_a9035_row9_col11\" class=\"data row9 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row9_col12\" class=\"data row9 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_a9035_row10_col0\" class=\"data row10 col0\" >Month</td>\n",
       "      <td id=\"T_a9035_row10_col1\" class=\"data row10 col1\" >int32</td>\n",
       "      <td id=\"T_a9035_row10_col2\" class=\"data row10 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row10_col3\" class=\"data row10 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row10_col4\" class=\"data row10 col4\" >12</td>\n",
       "      <td id=\"T_a9035_row10_col5\" class=\"data row10 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row10_col6\" class=\"data row10 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row10_col7\" class=\"data row10 col7\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row10_col8\" class=\"data row10 col8\" >11.000000</td>\n",
       "      <td id=\"T_a9035_row10_col9\" class=\"data row10 col9\" >5.523549</td>\n",
       "      <td id=\"T_a9035_row10_col10\" class=\"data row10 col10\" >3.448538</td>\n",
       "      <td id=\"T_a9035_row10_col11\" class=\"data row10 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row10_col12\" class=\"data row10 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_a9035_row11_col0\" class=\"data row11 col0\" >Day</td>\n",
       "      <td id=\"T_a9035_row11_col1\" class=\"data row11 col1\" >int32</td>\n",
       "      <td id=\"T_a9035_row11_col2\" class=\"data row11 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row11_col3\" class=\"data row11 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row11_col4\" class=\"data row11 col4\" >31</td>\n",
       "      <td id=\"T_a9035_row11_col5\" class=\"data row11 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row11_col6\" class=\"data row11 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row11_col7\" class=\"data row11 col7\" >1.000000</td>\n",
       "      <td id=\"T_a9035_row11_col8\" class=\"data row11 col8\" >31.000000</td>\n",
       "      <td id=\"T_a9035_row11_col9\" class=\"data row11 col9\" >15.727820</td>\n",
       "      <td id=\"T_a9035_row11_col10\" class=\"data row11 col10\" >8.799338</td>\n",
       "      <td id=\"T_a9035_row11_col11\" class=\"data row11 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row11_col12\" class=\"data row11 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_a9035_row12_col0\" class=\"data row12 col0\" >day_of_week</td>\n",
       "      <td id=\"T_a9035_row12_col1\" class=\"data row12 col1\" >int32</td>\n",
       "      <td id=\"T_a9035_row12_col2\" class=\"data row12 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row12_col3\" class=\"data row12 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row12_col4\" class=\"data row12 col4\" >7</td>\n",
       "      <td id=\"T_a9035_row12_col5\" class=\"data row12 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row12_col6\" class=\"data row12 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row12_col7\" class=\"data row12 col7\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row12_col8\" class=\"data row12 col8\" >6.000000</td>\n",
       "      <td id=\"T_a9035_row12_col9\" class=\"data row12 col9\" >2.998631</td>\n",
       "      <td id=\"T_a9035_row12_col10\" class=\"data row12 col10\" >1.999660</td>\n",
       "      <td id=\"T_a9035_row12_col11\" class=\"data row12 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row12_col12\" class=\"data row12 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_a9035_row13_col0\" class=\"data row13 col0\" >day_of_year</td>\n",
       "      <td id=\"T_a9035_row13_col1\" class=\"data row13 col1\" >int32</td>\n",
       "      <td id=\"T_a9035_row13_col2\" class=\"data row13 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row13_col3\" class=\"data row13 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row13_col4\" class=\"data row13 col4\" >7</td>\n",
       "      <td id=\"T_a9035_row13_col5\" class=\"data row13 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row13_col6\" class=\"data row13 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row13_col7\" class=\"data row13 col7\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row13_col8\" class=\"data row13 col8\" >6.000000</td>\n",
       "      <td id=\"T_a9035_row13_col9\" class=\"data row13 col9\" >2.998631</td>\n",
       "      <td id=\"T_a9035_row13_col10\" class=\"data row13 col10\" >1.999660</td>\n",
       "      <td id=\"T_a9035_row13_col11\" class=\"data row13 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row13_col12\" class=\"data row13 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_a9035_row14_col0\" class=\"data row14 col0\" >week_of_year</td>\n",
       "      <td id=\"T_a9035_row14_col1\" class=\"data row14 col1\" >int32</td>\n",
       "      <td id=\"T_a9035_row14_col2\" class=\"data row14 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row14_col3\" class=\"data row14 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row14_col4\" class=\"data row14 col4\" >53</td>\n",
       "      <td id=\"T_a9035_row14_col5\" class=\"data row14 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row14_col6\" class=\"data row14 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row14_col7\" class=\"data row14 col7\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row14_col8\" class=\"data row14 col8\" >52.000000</td>\n",
       "      <td id=\"T_a9035_row14_col9\" class=\"data row14 col9\" >26.413472</td>\n",
       "      <td id=\"T_a9035_row14_col10\" class=\"data row14 col10\" >15.059286</td>\n",
       "      <td id=\"T_a9035_row14_col11\" class=\"data row14 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row14_col12\" class=\"data row14 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_a9035_row15_col0\" class=\"data row15 col0\" >day_sin</td>\n",
       "      <td id=\"T_a9035_row15_col1\" class=\"data row15 col1\" >float32</td>\n",
       "      <td id=\"T_a9035_row15_col2\" class=\"data row15 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row15_col3\" class=\"data row15 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row15_col4\" class=\"data row15 col4\" >31</td>\n",
       "      <td id=\"T_a9035_row15_col5\" class=\"data row15 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row15_col6\" class=\"data row15 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row15_col7\" class=\"data row15 col7\" >0.017213</td>\n",
       "      <td id=\"T_a9035_row15_col8\" class=\"data row15 col8\" >0.508671</td>\n",
       "      <td id=\"T_a9035_row15_col9\" class=\"data row15 col9\" >0.264385</td>\n",
       "      <td id=\"T_a9035_row15_col10\" class=\"data row15 col10\" >0.144931</td>\n",
       "      <td id=\"T_a9035_row15_col11\" class=\"data row15 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row15_col12\" class=\"data row15 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_a9035_row16_col0\" class=\"data row16 col0\" >day_cos</td>\n",
       "      <td id=\"T_a9035_row16_col1\" class=\"data row16 col1\" >float32</td>\n",
       "      <td id=\"T_a9035_row16_col2\" class=\"data row16 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row16_col3\" class=\"data row16 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row16_col4\" class=\"data row16 col4\" >31</td>\n",
       "      <td id=\"T_a9035_row16_col5\" class=\"data row16 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row16_col6\" class=\"data row16 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row16_col7\" class=\"data row16 col7\" >0.860961</td>\n",
       "      <td id=\"T_a9035_row16_col8\" class=\"data row16 col8\" >0.999852</td>\n",
       "      <td id=\"T_a9035_row16_col9\" class=\"data row16 col9\" >0.952558</td>\n",
       "      <td id=\"T_a9035_row16_col10\" class=\"data row16 col10\" >0.041498</td>\n",
       "      <td id=\"T_a9035_row16_col11\" class=\"data row16 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row16_col12\" class=\"data row16 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_a9035_row17_col0\" class=\"data row17 col0\" >month_sin</td>\n",
       "      <td id=\"T_a9035_row17_col1\" class=\"data row17 col1\" >float32</td>\n",
       "      <td id=\"T_a9035_row17_col2\" class=\"data row17 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row17_col3\" class=\"data row17 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row17_col4\" class=\"data row17 col4\" >8</td>\n",
       "      <td id=\"T_a9035_row17_col5\" class=\"data row17 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row17_col6\" class=\"data row17 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row17_col7\" class=\"data row17 col7\" >-1.000000</td>\n",
       "      <td id=\"T_a9035_row17_col8\" class=\"data row17 col8\" >1.000000</td>\n",
       "      <td id=\"T_a9035_row17_col9\" class=\"data row17 col9\" >-0.004904</td>\n",
       "      <td id=\"T_a9035_row17_col10\" class=\"data row17 col10\" >0.705735</td>\n",
       "      <td id=\"T_a9035_row17_col11\" class=\"data row17 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row17_col12\" class=\"data row17 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_a9035_row18_col0\" class=\"data row18 col0\" >month_cos</td>\n",
       "      <td id=\"T_a9035_row18_col1\" class=\"data row18 col1\" >float32</td>\n",
       "      <td id=\"T_a9035_row18_col2\" class=\"data row18 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row18_col3\" class=\"data row18 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row18_col4\" class=\"data row18 col4\" >8</td>\n",
       "      <td id=\"T_a9035_row18_col5\" class=\"data row18 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row18_col6\" class=\"data row18 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row18_col7\" class=\"data row18 col7\" >-1.000000</td>\n",
       "      <td id=\"T_a9035_row18_col8\" class=\"data row18 col8\" >1.000000</td>\n",
       "      <td id=\"T_a9035_row18_col9\" class=\"data row18 col9\" >-0.002098</td>\n",
       "      <td id=\"T_a9035_row18_col10\" class=\"data row18 col10\" >0.708451</td>\n",
       "      <td id=\"T_a9035_row18_col11\" class=\"data row18 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row18_col12\" class=\"data row18 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_a9035_row19_col0\" class=\"data row19 col0\" >year_sin</td>\n",
       "      <td id=\"T_a9035_row19_col1\" class=\"data row19 col1\" >float32</td>\n",
       "      <td id=\"T_a9035_row19_col2\" class=\"data row19 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row19_col3\" class=\"data row19 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row19_col4\" class=\"data row19 col4\" >7</td>\n",
       "      <td id=\"T_a9035_row19_col5\" class=\"data row19 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row19_col6\" class=\"data row19 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row19_col7\" class=\"data row19 col7\" >-0.974928</td>\n",
       "      <td id=\"T_a9035_row19_col8\" class=\"data row19 col8\" >0.974928</td>\n",
       "      <td id=\"T_a9035_row19_col9\" class=\"data row19 col9\" >0.219063</td>\n",
       "      <td id=\"T_a9035_row19_col10\" class=\"data row19 col10\" >0.690902</td>\n",
       "      <td id=\"T_a9035_row19_col11\" class=\"data row19 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row19_col12\" class=\"data row19 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_a9035_row20_col0\" class=\"data row20 col0\" >year_cos</td>\n",
       "      <td id=\"T_a9035_row20_col1\" class=\"data row20 col1\" >float32</td>\n",
       "      <td id=\"T_a9035_row20_col2\" class=\"data row20 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row20_col3\" class=\"data row20 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row20_col4\" class=\"data row20 col4\" >4</td>\n",
       "      <td id=\"T_a9035_row20_col5\" class=\"data row20 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row20_col6\" class=\"data row20 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row20_col7\" class=\"data row20 col7\" >-0.900969</td>\n",
       "      <td id=\"T_a9035_row20_col8\" class=\"data row20 col8\" >1.000000</td>\n",
       "      <td id=\"T_a9035_row20_col9\" class=\"data row20 col9\" >-0.049945</td>\n",
       "      <td id=\"T_a9035_row20_col10\" class=\"data row20 col10\" >0.687085</td>\n",
       "      <td id=\"T_a9035_row20_col11\" class=\"data row20 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row20_col12\" class=\"data row20 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9035_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_a9035_row21_col0\" class=\"data row21 col0\" >Group</td>\n",
       "      <td id=\"T_a9035_row21_col1\" class=\"data row21 col1\" >int32</td>\n",
       "      <td id=\"T_a9035_row21_col2\" class=\"data row21 col2\" >0</td>\n",
       "      <td id=\"T_a9035_row21_col3\" class=\"data row21 col3\" >0.000000</td>\n",
       "      <td id=\"T_a9035_row21_col4\" class=\"data row21 col4\" >481</td>\n",
       "      <td id=\"T_a9035_row21_col5\" class=\"data row21 col5\" >0</td>\n",
       "      <td id=\"T_a9035_row21_col6\" class=\"data row21 col6\" >328680</td>\n",
       "      <td id=\"T_a9035_row21_col7\" class=\"data row21 col7\" >4.000000</td>\n",
       "      <td id=\"T_a9035_row21_col8\" class=\"data row21 col8\" >484.000000</td>\n",
       "      <td id=\"T_a9035_row21_col9\" class=\"data row21 col9\" >243.912377</td>\n",
       "      <td id=\"T_a9035_row21_col10\" class=\"data row21 col10\" >138.548865</td>\n",
       "      <td id=\"T_a9035_row21_col11\" class=\"data row21 col11\" >nan</td>\n",
       "      <td id=\"T_a9035_row21_col12\" class=\"data row21 col12\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1d207d4bcd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "describe_dataframe(df, name='Sales Data', nrows=0, plots=False, info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b962a-1958-4b6f-bcd9-73142d926e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5a38d996-fc5c-411f-bffb-b822e0fe1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with zero \n",
    "# Drop rows where 'dataset' is 'train' and 'num_sold' is NaN\n",
    "df = df[~((df['dataset'] == 'train') & (df['y'].isna()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ddd12d1d-ba9a-4866-a11e-34d593d8d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a logarithmic transformation to 'num_sold' to reduce skewness and stabilize variance.\n",
    "# This transformation helps to handle outliers by compressing large values, making the data \n",
    "# more suitable for statistical analysis and modeling.\n",
    "df['y'] = np.log(df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab706442-bd68-4afd-a436-cc883e19576b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "69156aa2-4e9b-4dd3-b8b7-c955dac099f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables (country, store, product) as dummy variables\n",
    "# Avoiding the dummy variable trap by dropping the first category in each column\n",
    "\n",
    "df = pd.get_dummies(df, columns=['country', 'store', 'product'], dtype=int, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ba286-de3f-4725-a98d-d7a09430ad60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ea5e229d-3501-4cd5-88e6-f9cd5f6d2b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(24,8)})\n",
    "# ax=sns.lineplot(data=df_train,x='date',y='y',hue='product')\n",
    "# ax.axes.set_title(\"\\nBasic Time Series of Sales\\n\",fontsize=20);`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8ab92afe-b0b1-4bcf-899d-09a93a10e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(24,8)})\n",
    "# ax=sns.lineplot(data=df_train,x='Day of Year',y='y')\n",
    "# ax.axes.set_title(\"\\nBasic Time Series of Sales\\n\",fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "809f5a96-8033-41f3-a2b4-c80ebca1207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(24,8)})\n",
    "# ax=sns.lineplot(data=df_train,x='Week of Year',y='y')\n",
    "# ax.axes.set_title(\"\\nBasic Time Series of Sales\\n\",fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4e519bab-dc82-4f69-9f00-e3ed80784f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(24,8)})\n",
    "# ax=sns.lineplot(data=df_train,x='Month',y='y')\n",
    "# ax.axes.set_title(\"\\nBasic Time Series of Sales\\n\",fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "868ad7c8-b818-477c-a67c-cfc65738c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(24,8)})\n",
    "# ax=sns.lineplot(data=df_train,x='Year',y='y', hue='country')\n",
    "# ax.axes.set_title(\"\\nBasic Time Series of Sales\\n\",fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0722c398-32e3-4858-9724-04c42f56775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(24,8)})\n",
    "# ax=sns.lineplot(data=df_train,x='Day of Year Sin',y='y')\n",
    "# ax.axes.set_title(\"\\nBasic Time Series of Sales\\n\",fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee417c-537f-4ee5-b8e6-16caac18a081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b2ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b98f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bd930c7c-10c7-4080-98c8-d7097d893708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the training dataset into train and validation sets\n",
    "# Ensuring the split preserves the time series order\n",
    "\n",
    "# Separate train and test datasets\n",
    "train_df = df[df['dataset'] == 'train'].drop(columns=['dataset'], errors='ignore')\n",
    "test_df = df[df['dataset'] == 'test'].drop(columns=['dataset'], errors='ignore')\n",
    "\n",
    "\n",
    "# Drop unnecessary columns from both datasets\n",
    "train_df = train_df.drop(columns=['id'], errors='ignore')\n",
    "test_df = test_df.drop(columns=['y'], errors='ignore')\n",
    "\n",
    "# Sort training data by date to preserve time series order\n",
    "train_df = train_df.sort_values(by='date')\n",
    "\n",
    "# Let's define a split date or index\n",
    "train_size = int(len(train_df) * 0.86)  # 80% for training\n",
    "df_train = train_df.iloc[:train_size]\n",
    "df_test = train_df.iloc[train_size:]\n",
    "\n",
    "# Separate features and target\n",
    "x_train = df_train.drop(['y'], axis=1)\n",
    "y_train = df_train['y']\n",
    "x_test = df_test.drop(['y'], axis=1)\n",
    "y_test = df_test['y']\n",
    "\n",
    "\n",
    "\n",
    "# Drop the 'date' column after feature extraction\n",
    "x_train = x_train.drop(columns=['date'], errors='ignore')\n",
    "x_test = x_test.drop(columns=['date'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f1d3e-8409-46a2-a2f0-077d355d28c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cfeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7943187c-1a08-47f9-abf6-a2ca00d987d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:08:41,705] A new study created in memory with name: no-name-b0ff5dcf-fb7b-4554-b9d2-1cd4e7d3c3d9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:08:48,255] Trial 0 finished with value: 1.3591522997899843 and parameters: {'num_leaves': 58, 'learning_rate': 0.00030596264725014997, 'n_estimators': 600, 'subsample': 0.5003847159379098, 'colsample_bytree': 0.7065911405314493}. Best is trial 0 with value: 1.3591522997899843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:08:57,483] Trial 1 finished with value: 1.3498001122262162 and parameters: {'num_leaves': 122, 'learning_rate': 0.000279028617250203, 'n_estimators': 500, 'subsample': 0.7554377981170751, 'colsample_bytree': 0.9587729325484207}. Best is trial 1 with value: 1.3498001122262162.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001528 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:09:00,904] Trial 2 finished with value: 0.8440817776288637 and parameters: {'num_leaves': 98, 'learning_rate': 0.0039497299786651965, 'n_estimators': 200, 'subsample': 0.9351274899126472, 'colsample_bytree': 0.7170645835801746}. Best is trial 2 with value: 0.8440817776288637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000537 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:09:06,062] Trial 3 finished with value: 1.4153765283207749 and parameters: {'num_leaves': 27, 'learning_rate': 0.00018642415357011336, 'n_estimators': 600, 'subsample': 0.5472755964807138, 'colsample_bytree': 0.7949640921092485}. Best is trial 2 with value: 0.8440817776288637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:09:11,716] Trial 4 finished with value: 0.11157810133687203 and parameters: {'num_leaves': 60, 'learning_rate': 0.05723166838626199, 'n_estimators': 600, 'subsample': 0.7417938695380206, 'colsample_bytree': 0.545419126046581}. Best is trial 4 with value: 0.11157810133687203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:09:22,326] Trial 5 finished with value: 0.11047240935358875 and parameters: {'num_leaves': 79, 'learning_rate': 0.0239645880455166, 'n_estimators': 800, 'subsample': 0.5748724256805376, 'colsample_bytree': 0.654043866476663}. Best is trial 5 with value: 0.11047240935358875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:09:38,868] Trial 6 finished with value: 1.3165370358274582 and parameters: {'num_leaves': 120, 'learning_rate': 0.00028238247732873434, 'n_estimators': 900, 'subsample': 0.5156524527419994, 'colsample_bytree': 0.631529021716627}. Best is trial 5 with value: 0.11047240935358875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:09:42,535] Trial 7 finished with value: 1.4981175304354395 and parameters: {'num_leaves': 51, 'learning_rate': 0.00013713329682619628, 'n_estimators': 300, 'subsample': 0.6151406006623192, 'colsample_bytree': 0.7965723665478082}. Best is trial 5 with value: 0.11047240935358875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:09:48,622] Trial 8 finished with value: 0.3128024437435084 and parameters: {'num_leaves': 116, 'learning_rate': 0.00563848339920459, 'n_estimators': 300, 'subsample': 0.9190303932349653, 'colsample_bytree': 0.9168614086503426}. Best is trial 5 with value: 0.11047240935358875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001826 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:09:52,210] Trial 9 finished with value: 0.11400204148251693 and parameters: {'num_leaves': 48, 'learning_rate': 0.04389313986238294, 'n_estimators': 300, 'subsample': 0.9059729604514164, 'colsample_bytree': 0.6522548921967508}. Best is trial 5 with value: 0.11047240935358875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:10:07,465] Trial 10 finished with value: 0.11362782345998983 and parameters: {'num_leaves': 86, 'learning_rate': 0.016616025889245937, 'n_estimators': 1000, 'subsample': 0.6909607087715379, 'colsample_bytree': 0.5054115635979848}. Best is trial 5 with value: 0.11047240935358875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:10:16,225] Trial 11 finished with value: 0.1125517299642511 and parameters: {'num_leaves': 78, 'learning_rate': 0.09329896990182203, 'n_estimators': 800, 'subsample': 0.8195493021745273, 'colsample_bytree': 0.5170760190765897}. Best is trial 5 with value: 0.11047240935358875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:10:26,093] Trial 12 finished with value: 0.11205301315907079 and parameters: {'num_leaves': 68, 'learning_rate': 0.018995900687655727, 'n_estimators': 700, 'subsample': 0.6651262008613955, 'colsample_bytree': 0.5880279273163184}. Best is trial 5 with value: 0.11047240935358875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:10:32,040] Trial 13 finished with value: 0.11104371164923081 and parameters: {'num_leaves': 30, 'learning_rate': 0.09709762919721009, 'n_estimators': 800, 'subsample': 0.8065399663563843, 'colsample_bytree': 0.5772591738099457}. Best is trial 5 with value: 0.11047240935358875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:10:38,286] Trial 14 finished with value: 0.12177821768561287 and parameters: {'num_leaves': 17, 'learning_rate': 0.01740757986833072, 'n_estimators': 800, 'subsample': 0.8354785754812157, 'colsample_bytree': 0.6270402659368634}. Best is trial 5 with value: 0.11047240935358875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:10:50,296] Trial 15 finished with value: 0.7130780951717218 and parameters: {'num_leaves': 36, 'learning_rate': 0.0011347640098913027, 'n_estimators': 1000, 'subsample': 0.5933276527526785, 'colsample_bytree': 0.677819597107236}. Best is trial 5 with value: 0.11047240935358875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:11:02,669] Trial 16 finished with value: 0.11033385267729535 and parameters: {'num_leaves': 92, 'learning_rate': 0.03230057220755551, 'n_estimators': 800, 'subsample': 0.8384737368308744, 'colsample_bytree': 0.5790323473317228}. Best is trial 16 with value: 0.11033385267729535.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000771 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001564 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:11:12,696] Trial 17 finished with value: 0.12455815993120688 and parameters: {'num_leaves': 98, 'learning_rate': 0.008758289806391286, 'n_estimators': 500, 'subsample': 0.8647394834096163, 'colsample_bytree': 0.8085935008854861}. Best is trial 16 with value: 0.11033385267729535.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000522 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000895 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001243 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:11:30,396] Trial 18 finished with value: 0.7462204826668773 and parameters: {'num_leaves': 100, 'learning_rate': 0.001330456009390203, 'n_estimators': 900, 'subsample': 0.9712484860340498, 'colsample_bytree': 0.6076709897264065}. Best is trial 16 with value: 0.11033385267729535.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:11:41,403] Trial 19 finished with value: 0.10955227161752122 and parameters: {'num_leaves': 90, 'learning_rate': 0.028631316607835845, 'n_estimators': 700, 'subsample': 0.7483541535600521, 'colsample_bytree': 0.7501637716908929}. Best is trial 19 with value: 0.10955227161752122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:11:56,436] Trial 20 finished with value: 0.44153966622118423 and parameters: {'num_leaves': 109, 'learning_rate': 0.0019869821784813794, 'n_estimators': 700, 'subsample': 0.7637265807445338, 'colsample_bytree': 0.8869665186554825}. Best is trial 19 with value: 0.10955227161752122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:12:07,039] Trial 21 finished with value: 0.11096680269917955 and parameters: {'num_leaves': 84, 'learning_rate': 0.03412252462922496, 'n_estimators': 700, 'subsample': 0.7001551005029879, 'colsample_bytree': 0.7495853529166842}. Best is trial 19 with value: 0.10955227161752122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000574 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:12:23,230] Trial 22 finished with value: 0.1130055767590491 and parameters: {'num_leaves': 89, 'learning_rate': 0.00990799480715604, 'n_estimators': 900, 'subsample': 0.6285098722838609, 'colsample_bytree': 0.6786033900596655}. Best is trial 19 with value: 0.10955227161752122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:12:34,383] Trial 23 finished with value: 0.11029125206745156 and parameters: {'num_leaves': 74, 'learning_rate': 0.028230660069915377, 'n_estimators': 800, 'subsample': 0.8723732215165141, 'colsample_bytree': 0.8524101390775347}. Best is trial 19 with value: 0.10955227161752122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:12:45,707] Trial 24 finished with value: 0.11348368920087688 and parameters: {'num_leaves': 73, 'learning_rate': 0.009868565167557138, 'n_estimators': 700, 'subsample': 0.8620638888334505, 'colsample_bytree': 0.8567585089066903}. Best is trial 19 with value: 0.10955227161752122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:12:53,624] Trial 25 finished with value: 0.11013483156340284 and parameters: {'num_leaves': 106, 'learning_rate': 0.05048592696020243, 'n_estimators': 400, 'subsample': 0.7819157114650993, 'colsample_bytree': 0.8505325587130811}. Best is trial 19 with value: 0.10955227161752122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:13:00,819] Trial 26 finished with value: 0.10973396193959152 and parameters: {'num_leaves': 107, 'learning_rate': 0.058129448907374266, 'n_estimators': 400, 'subsample': 0.7786387238641987, 'colsample_bytree': 0.8480583231734742}. Best is trial 19 with value: 0.10955227161752122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:13:07,567] Trial 27 finished with value: 0.10998916480365849 and parameters: {'num_leaves': 107, 'learning_rate': 0.06268393447768296, 'n_estimators': 400, 'subsample': 0.779224284933554, 'colsample_bytree': 0.9537877023105363}. Best is trial 19 with value: 0.10955227161752122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:13:10,235] Trial 28 finished with value: 0.11136457590564795 and parameters: {'num_leaves': 128, 'learning_rate': 0.07309970717247657, 'n_estimators': 100, 'subsample': 0.7210619590895609, 'colsample_bytree': 0.9633427098350104}. Best is trial 19 with value: 0.10955227161752122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:13:18,581] Trial 29 finished with value: 1.2126848831687704 and parameters: {'num_leaves': 111, 'learning_rate': 0.0006370253199256776, 'n_estimators': 400, 'subsample': 0.7898365147578714, 'colsample_bytree': 0.9245433898536943}. Best is trial 19 with value: 0.10955227161752122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:13:25,097] Trial 30 finished with value: 0.11136126054869973 and parameters: {'num_leaves': 103, 'learning_rate': 0.0586469330660691, 'n_estimators': 400, 'subsample': 0.6543186971039305, 'colsample_bytree': 0.7449691690286546}. Best is trial 19 with value: 0.10955227161752122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001852 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:13:31,853] Trial 31 finished with value: 0.11074998912693637 and parameters: {'num_leaves': 107, 'learning_rate': 0.05269109601089493, 'n_estimators': 400, 'subsample': 0.7860268867102572, 'colsample_bytree': 0.8416804911019953}. Best is trial 19 with value: 0.10955227161752122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000817 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:13:39,896] Trial 32 finished with value: 0.10992214123513673 and parameters: {'num_leaves': 96, 'learning_rate': 0.04151757238154457, 'n_estimators': 500, 'subsample': 0.7371495057870961, 'colsample_bytree': 0.9251184931584331}. Best is trial 19 with value: 0.10955227161752122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:13:49,394] Trial 33 finished with value: 0.11209974116480119 and parameters: {'num_leaves': 97, 'learning_rate': 0.013819004808100508, 'n_estimators': 500, 'subsample': 0.7639130524515255, 'colsample_bytree': 0.9934738643294859}. Best is trial 19 with value: 0.10955227161752122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000816 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 47572, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.934451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 448\n",
      "[LightGBM] [Info] Number of data points in the train set: 95142, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.950947\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 524\n",
      "[LightGBM] [Info] Number of data points in the train set: 142712, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.955299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 15:14:00,164] Trial 34 finished with value: 0.10916081923210903 and parameters: {'num_leaves': 115, 'learning_rate': 0.03504514673967671, 'n_estimators': 600, 'subsample': 0.7318909906459061, 'colsample_bytree': 0.9155705430641332}. Best is trial 34 with value: 0.10916081923210903.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'num_leaves': 115, 'learning_rate': 0.03504514673967671, 'n_estimators': 600, 'subsample': 0.7318909906459061, 'colsample_bytree': 0.9155705430641332}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# Define the objective function for Optuna, which evaluates LightGBM's performance\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameter values to be optimized by Optuna\n",
    "    params = {\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 128),  # Maximum leaves in one tree\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),  # Learning rate for gradient boosting\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000, step=100),  # Number of boosting rounds\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),  # Fraction of samples used per iteration\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),  # Fraction of features used for each tree\n",
    "        \"random_state\": 42,  # Ensures reproducibility\n",
    "    }\n",
    "    \n",
    "    # Use TimeSeriesSplit to preserve temporal order during cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    model = LGBMRegressor(**params)  # Initialize the LightGBM model with current parameters\n",
    "    \n",
    "    errors = []  # List to store validation errors for each fold\n",
    "    for train_index, val_index in tscv.split(x_train):  # Split training data for cross-validation\n",
    "        x_train_cv, X_val_cv = x_train.iloc[train_index], x_train.iloc[val_index]  # Train/validation splits\n",
    "        y_train_cv, y_val_cv = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        model.fit(x_train_cv, y_train_cv)  # Train the model on the current fold\n",
    "        y_val_pred = model.predict(X_val_cv)  # Predict on the validation set\n",
    "        errors.append(mean_squared_error(y_val_cv, y_val_pred))  # Calculate and store MSE\n",
    "    \n",
    "    # Calculate the Root Mean Squared Error (RMSE) for the current trial\n",
    "    rmse = np.sqrt(np.mean(errors))\n",
    "    return rmse  # Return RMSE as the objective value for Optuna to minimize\n",
    "\n",
    "# Initialize an Optuna study for hyperparameter optimization\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize RMSE\n",
    "study.optimize(objective, n_trials=35)  # Optimize the objective function over 30 trials\n",
    "\n",
    "# Print the best hyperparameters found during optimization\n",
    "print(\"Best trial:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb54ec-48cd-4b1f-b769-0075e8c108d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b98edc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b2beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "74800ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'num_leaves': 115, 'learning_rate': 0.03504514673967671, 'n_estimators': 600, 'subsample': 0.7318909906459061, 'colsample_bytree': 0.9155705430641332}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 565\n",
      "[LightGBM] [Info] Number of data points in the train set: 190282, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.937273\n",
      "Final test RMSE: 0.08078035836239862\n",
      "Final test MAPE: 0.01220820001448394\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best hyperparameters found during Optuna optimization\n",
    "best_params = study.best_trial.params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Instantiate the LightGBM model using the best hyperparameters\n",
    "final_model = LGBMRegressor(**best_params)\n",
    "\n",
    "# Train the model on the training dataset\n",
    "final_model.fit(x_train, y_train)\n",
    "\n",
    "# Predict the target variable for the test dataset\n",
    "y_test_pred = final_model.predict(x_test)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for model evaluation\n",
    "# RMSE indicates the average magnitude of prediction errors, lower is better\n",
    "final_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "print(\"Final test RMSE:\", final_test_rmse)\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE) for model evaluation\n",
    "# MAPE shows the average percentage error between predicted and actual values, lower is better\n",
    "final_test_mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "print(\"Final test MAPE:\", final_test_mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2253c8a2-3856-4d21-a14f-df331c8504af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'num_leaves': 115, 'learning_rate': 0.03504514673967671, 'n_estimators': 600, 'subsample': 0.7318909906459061, 'colsample_bytree': 0.9155705430641332}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 558\n",
      "[LightGBM] [Info] Number of data points in the train set: 221259, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 5.925537\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(colsample_bytree=0.9155705430641332,\n",
       "              learning_rate=0.03504514673967671, n_estimators=600,\n",
       "              num_leaves=115, subsample=0.7318909906459061)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LGBMRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMRegressor(colsample_bytree=0.9155705430641332,\n",
       "              learning_rate=0.03504514673967671, n_estimators=600,\n",
       "              num_leaves=115, subsample=0.7318909906459061)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(colsample_bytree=0.9155705430641332,\n",
       "              learning_rate=0.03504514673967671, n_estimators=600,\n",
       "              num_leaves=115, subsample=0.7318909906459061)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate features (X) and target variable (y) from the training data\n",
    "# Dropping 'num_sold' as it's the target variable\n",
    "# Dropping 'date' to avoid any unintended leakage since it is not used as a feature\n",
    "X = train_df.drop(columns=['y', 'date']).copy()\n",
    "y = train_df['y'].copy()\n",
    "\n",
    "# Retrieve the best hyperparameters from the Optuna study\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Instantiate the LightGBM model with the optimal hyperparameters\n",
    "best_model = LGBMRegressor(**best_params)\n",
    "\n",
    "# Train the model using all available training data (X, y)\n",
    "best_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c6d94ded-695e-4201-9588-14be6a3ef7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "test_features = test_df.drop(columns=['id', 'date'], errors='ignore')  # Drop unnecessary columns\n",
    "test_features = test_features.reindex(columns=x_train.columns, fill_value=0)  # Align columns with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3a203565",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = best_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8e77c9fb-49f6-40b9-ab84-6bf1d9f0f6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98550, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>num_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230130</td>\n",
       "      <td>173.877260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230131</td>\n",
       "      <td>964.990713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>230132</td>\n",
       "      <td>741.810135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230133</td>\n",
       "      <td>384.992800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230134</td>\n",
       "      <td>480.034632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    num_sold\n",
       "0  230130  173.877260\n",
       "1  230131  964.990713\n",
       "2  230132  741.810135\n",
       "3  230133  384.992800\n",
       "4  230134  480.034632"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv(submission_path)\n",
    "sub[target_variable] = np.expm1(y_test_pred)\n",
    "sub.to_csv('Predictions_LGBM.csv', index=False)\n",
    "print(sub.shape)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc755366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
