{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "87ecc9be-8eb3-42cf-a103-f882ac54b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import socket\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import holidays\n",
    "import pycountry\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from termcolor import colored\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "3d21237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "# from catboost.utils import get_gpu_device_count\n",
    "\n",
    "# # Check available GPUs\n",
    "# gpu_count = get_gpu_device_count()\n",
    "# if gpu_count > 0:\n",
    "#     print(f\"GPU is available with {gpu_count} GPU(s).\")\n",
    "# else:\n",
    "#     print(\"No GPU detected. Ensure your setup is correct.\")\n",
    "\n",
    "# # Test CatBoost with GPU\n",
    "# try:\n",
    "#     model = CatBoostClassifier(task_type=\"GPU\", devices='0')  # Specify GPU\n",
    "#     device = 'cuda'\n",
    "#     print(\"CatBoost can use the GPU.\")\n",
    "        \n",
    "# except Exception as e:\n",
    "#     device = 'cpu'\n",
    "#     print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "2ee7c11a-7155-4d25-a8de-24e6c05d0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare important variables\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "target_variable = 'num_sold'\n",
    "SEED = 69\n",
    "skip_hypertuning = True\n",
    "cat_encoder_type = 'OneHotEncoder' #OneHotEncoder, FrequencyEncoder, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "2a6afd61-d48c-4d0a-8272-39ad990662f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframes(path1: str, path2: str, format1: str = \"csv\", format2: str = \"csv\") -> tuple:\n",
    "\n",
    "    loaders = {\n",
    "        \"csv\": pd.read_csv,\n",
    "        \"excel\": pd.read_excel,\n",
    "        \"json\": pd.read_json,\n",
    "    }\n",
    "\n",
    "    if format1 not in loaders or format2 not in loaders:\n",
    "        raise ValueError(\"Unsupported format. Supported formats: 'csv', 'excel', 'json'.\")\n",
    "\n",
    "    # Load the dataframes using appropriate loaders\n",
    "    df1 = loaders[format1](path1)\n",
    "    df2 = loaders[format2](path2)\n",
    "\n",
    "    print(f\"Loading data from {os.getcwd()}\")\n",
    "    \n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "0fce9fbd-384a-4fac-8bbf-3eb68d30d172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from C:\\Users\\zchodaniecky\\OneDrive - Franklin Templeton\\Documents\\Python\\Kaggle\\Playground Series\\s5e1 - Sticker Sales\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = load_dataframes(train_path,test_path)\n",
    "\n",
    "df_train = df_train.rename(columns={target_variable: 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "3d828194-d954-4d0e-b48a-020dcd4d6ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_kde_grid(df_train,n_cols=4,figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "37f42554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_kde_grid(df_solve,n_cols=4,figsize=(9,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "81cb01aa-4360-417b-9fc7-4070a5384573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_cols(df: pd.DataFrame):\n",
    "    num_cols = df.select_dtypes(include=['number']).columns.to_list()\n",
    "    \n",
    "    return num_cols\n",
    "\n",
    "def get_cat_cols(df: pd.DataFrame):\n",
    "    cat_cols = df.select_dtypes(include=['object', 'string', 'category']).columns.tolist()  \n",
    "\n",
    "    return cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f68956-e3b5-45ab-a4d1-5be3e5a18fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "7a363157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_dataframe(df_to_use: pd.DataFrame, name: str = 'DataFrame', nrows: int = 3, plots: bool = False, info: bool = True) -> None:\n",
    "    '''\n",
    "    Function to describe the DataFrame with summary statistics, missing value count,\n",
    "    unique value count, and duplicate count. It also displays plots for missing and unique values.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame to describe.\n",
    "    - name: The name to display in the summary (default is 'DataFrame').\n",
    "    - nrows: The number of rows to display from the top and bottom (default is 3).\n",
    "    - plots: Whether to display bar plots for missing and unique values (default is False).\n",
    "    - info: Whether to display the styled DataFrame (default is True).\n",
    "    '''\n",
    "\n",
    "    df = df_to_use.copy()   \n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    inf = pd.DataFrame(df.dtypes).reset_index().rename(columns={'index':'column', 0:'type'})\n",
    "    \n",
    "    # Missing values\n",
    "    df_missing = pd.DataFrame(df.isnull().sum()).reset_index().rename(columns={'index':'column', 0:'missing'})\n",
    "    df_missing['pct_missing'] = (df_missing['missing'] / df.shape[0]) * 100\n",
    "    \n",
    "    # Unique values\n",
    "    df_unique = pd.DataFrame(df.nunique()).reset_index().rename(columns={'index':'column', 0:'unique'})\n",
    "    \n",
    "    # Combine summary information\n",
    "    inf['missing'] = df_missing['missing']\n",
    "    inf['pct_missing'] = df_missing['pct_missing']\n",
    "    inf['unique'] = df_unique['unique']\n",
    "    inf['duplicate'] = df.duplicated().sum()\n",
    "    inf['count'] = df.shape[0]\n",
    "\n",
    "    # Descriptive statistics\n",
    "    desc = pd.DataFrame(df.describe(include='all').transpose())\n",
    "    if 'min' in desc.columns.tolist():\n",
    "        inf['min'] = desc['min'].values\n",
    "        inf['max'] = desc['max'].values\n",
    "        inf['avg'] = desc['mean'].values\n",
    "        inf['std dev'] = desc['std'].values\n",
    "    if 'top' in desc.columns.tolist():\n",
    "        inf['top value'] = desc['top'].values\n",
    "        inf['Freq'] = desc['freq'].values \n",
    "    \n",
    "    # Display styled DataFrame\n",
    "    if info:\n",
    "        display(inf.style.background_gradient(subset=['missing','pct_missing'], cmap='Reds').background_gradient(subset='unique', cmap='Greens'))\n",
    "\n",
    "    if nrows != 0 :\n",
    "        # Display top and bottom nrows of the DataFrame\n",
    "        print(f\"\\n---------- {name} Overview ----------:\")\n",
    "        print(f\"{name} has {df.shape[0]} rows and {df.shape[1]} columns\\n\")\n",
    "        display(df.head(nrows))\n",
    "        display(df.tail(nrows))\n",
    "    \n",
    "    # Plot missing values if any\n",
    "    if plots and df_missing['missing'].sum() > 0:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(24, 5))\n",
    "        sns.barplot(df_missing[df_missing['missing'] > 0], x='column', y='missing', ax=ax)\n",
    "        ax.set_title(f'{name} missing Values') \n",
    "        ax.bar_label(ax.containers[0])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "        # Plot unique values\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(24, 5))\n",
    "        sns.barplot(df_unique[df_unique['unique'] > 0], x='column', y='unique', ax=ax)\n",
    "        ax.set_title(f'{name} Unique Values')\n",
    "        ax.bar_label(ax.containers[0])\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "da182033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5e704_row0_col2, #T_5e704_row0_col3, #T_5e704_row1_col2, #T_5e704_row1_col3, #T_5e704_row2_col2, #T_5e704_row2_col3, #T_5e704_row3_col2, #T_5e704_row3_col3, #T_5e704_row4_col2, #T_5e704_row4_col3 {\n",
       "  background-color: #fff5f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e704_row0_col4 {\n",
       "  background-color: #00441b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5e704_row1_col4 {\n",
       "  background-color: #f6fcf4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e704_row2_col4, #T_5e704_row3_col4, #T_5e704_row4_col4 {\n",
       "  background-color: #f7fcf5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e704_row5_col2, #T_5e704_row5_col3 {\n",
       "  background-color: #67000d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5e704_row5_col4 {\n",
       "  background-color: #f5fbf2;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5e704\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5e704_level0_col0\" class=\"col_heading level0 col0\" >column</th>\n",
       "      <th id=\"T_5e704_level0_col1\" class=\"col_heading level0 col1\" >type</th>\n",
       "      <th id=\"T_5e704_level0_col2\" class=\"col_heading level0 col2\" >missing</th>\n",
       "      <th id=\"T_5e704_level0_col3\" class=\"col_heading level0 col3\" >pct_missing</th>\n",
       "      <th id=\"T_5e704_level0_col4\" class=\"col_heading level0 col4\" >unique</th>\n",
       "      <th id=\"T_5e704_level0_col5\" class=\"col_heading level0 col5\" >duplicate</th>\n",
       "      <th id=\"T_5e704_level0_col6\" class=\"col_heading level0 col6\" >count</th>\n",
       "      <th id=\"T_5e704_level0_col7\" class=\"col_heading level0 col7\" >min</th>\n",
       "      <th id=\"T_5e704_level0_col8\" class=\"col_heading level0 col8\" >max</th>\n",
       "      <th id=\"T_5e704_level0_col9\" class=\"col_heading level0 col9\" >avg</th>\n",
       "      <th id=\"T_5e704_level0_col10\" class=\"col_heading level0 col10\" >std dev</th>\n",
       "      <th id=\"T_5e704_level0_col11\" class=\"col_heading level0 col11\" >top value</th>\n",
       "      <th id=\"T_5e704_level0_col12\" class=\"col_heading level0 col12\" >Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5e704_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5e704_row0_col0\" class=\"data row0 col0\" >id</td>\n",
       "      <td id=\"T_5e704_row0_col1\" class=\"data row0 col1\" >int64</td>\n",
       "      <td id=\"T_5e704_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_5e704_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "      <td id=\"T_5e704_row0_col4\" class=\"data row0 col4\" >230130</td>\n",
       "      <td id=\"T_5e704_row0_col5\" class=\"data row0 col5\" >0</td>\n",
       "      <td id=\"T_5e704_row0_col6\" class=\"data row0 col6\" >230130</td>\n",
       "      <td id=\"T_5e704_row0_col7\" class=\"data row0 col7\" >0.000000</td>\n",
       "      <td id=\"T_5e704_row0_col8\" class=\"data row0 col8\" >230129.000000</td>\n",
       "      <td id=\"T_5e704_row0_col9\" class=\"data row0 col9\" >115064.500000</td>\n",
       "      <td id=\"T_5e704_row0_col10\" class=\"data row0 col10\" >66432.953062</td>\n",
       "      <td id=\"T_5e704_row0_col11\" class=\"data row0 col11\" >nan</td>\n",
       "      <td id=\"T_5e704_row0_col12\" class=\"data row0 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e704_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5e704_row1_col0\" class=\"data row1 col0\" >date</td>\n",
       "      <td id=\"T_5e704_row1_col1\" class=\"data row1 col1\" >object</td>\n",
       "      <td id=\"T_5e704_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_5e704_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "      <td id=\"T_5e704_row1_col4\" class=\"data row1 col4\" >2557</td>\n",
       "      <td id=\"T_5e704_row1_col5\" class=\"data row1 col5\" >0</td>\n",
       "      <td id=\"T_5e704_row1_col6\" class=\"data row1 col6\" >230130</td>\n",
       "      <td id=\"T_5e704_row1_col7\" class=\"data row1 col7\" >nan</td>\n",
       "      <td id=\"T_5e704_row1_col8\" class=\"data row1 col8\" >nan</td>\n",
       "      <td id=\"T_5e704_row1_col9\" class=\"data row1 col9\" >nan</td>\n",
       "      <td id=\"T_5e704_row1_col10\" class=\"data row1 col10\" >nan</td>\n",
       "      <td id=\"T_5e704_row1_col11\" class=\"data row1 col11\" >2010-01-01</td>\n",
       "      <td id=\"T_5e704_row1_col12\" class=\"data row1 col12\" >90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e704_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5e704_row2_col0\" class=\"data row2 col0\" >country</td>\n",
       "      <td id=\"T_5e704_row2_col1\" class=\"data row2 col1\" >object</td>\n",
       "      <td id=\"T_5e704_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_5e704_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
       "      <td id=\"T_5e704_row2_col4\" class=\"data row2 col4\" >6</td>\n",
       "      <td id=\"T_5e704_row2_col5\" class=\"data row2 col5\" >0</td>\n",
       "      <td id=\"T_5e704_row2_col6\" class=\"data row2 col6\" >230130</td>\n",
       "      <td id=\"T_5e704_row2_col7\" class=\"data row2 col7\" >nan</td>\n",
       "      <td id=\"T_5e704_row2_col8\" class=\"data row2 col8\" >nan</td>\n",
       "      <td id=\"T_5e704_row2_col9\" class=\"data row2 col9\" >nan</td>\n",
       "      <td id=\"T_5e704_row2_col10\" class=\"data row2 col10\" >nan</td>\n",
       "      <td id=\"T_5e704_row2_col11\" class=\"data row2 col11\" >Canada</td>\n",
       "      <td id=\"T_5e704_row2_col12\" class=\"data row2 col12\" >38355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e704_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_5e704_row3_col0\" class=\"data row3 col0\" >store</td>\n",
       "      <td id=\"T_5e704_row3_col1\" class=\"data row3 col1\" >object</td>\n",
       "      <td id=\"T_5e704_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_5e704_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "      <td id=\"T_5e704_row3_col4\" class=\"data row3 col4\" >3</td>\n",
       "      <td id=\"T_5e704_row3_col5\" class=\"data row3 col5\" >0</td>\n",
       "      <td id=\"T_5e704_row3_col6\" class=\"data row3 col6\" >230130</td>\n",
       "      <td id=\"T_5e704_row3_col7\" class=\"data row3 col7\" >nan</td>\n",
       "      <td id=\"T_5e704_row3_col8\" class=\"data row3 col8\" >nan</td>\n",
       "      <td id=\"T_5e704_row3_col9\" class=\"data row3 col9\" >nan</td>\n",
       "      <td id=\"T_5e704_row3_col10\" class=\"data row3 col10\" >nan</td>\n",
       "      <td id=\"T_5e704_row3_col11\" class=\"data row3 col11\" >Discount Stickers</td>\n",
       "      <td id=\"T_5e704_row3_col12\" class=\"data row3 col12\" >76710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e704_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_5e704_row4_col0\" class=\"data row4 col0\" >product</td>\n",
       "      <td id=\"T_5e704_row4_col1\" class=\"data row4 col1\" >object</td>\n",
       "      <td id=\"T_5e704_row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "      <td id=\"T_5e704_row4_col3\" class=\"data row4 col3\" >0.000000</td>\n",
       "      <td id=\"T_5e704_row4_col4\" class=\"data row4 col4\" >5</td>\n",
       "      <td id=\"T_5e704_row4_col5\" class=\"data row4 col5\" >0</td>\n",
       "      <td id=\"T_5e704_row4_col6\" class=\"data row4 col6\" >230130</td>\n",
       "      <td id=\"T_5e704_row4_col7\" class=\"data row4 col7\" >nan</td>\n",
       "      <td id=\"T_5e704_row4_col8\" class=\"data row4 col8\" >nan</td>\n",
       "      <td id=\"T_5e704_row4_col9\" class=\"data row4 col9\" >nan</td>\n",
       "      <td id=\"T_5e704_row4_col10\" class=\"data row4 col10\" >nan</td>\n",
       "      <td id=\"T_5e704_row4_col11\" class=\"data row4 col11\" >Holographic Goose</td>\n",
       "      <td id=\"T_5e704_row4_col12\" class=\"data row4 col12\" >46026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e704_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_5e704_row5_col0\" class=\"data row5 col0\" >y</td>\n",
       "      <td id=\"T_5e704_row5_col1\" class=\"data row5 col1\" >float64</td>\n",
       "      <td id=\"T_5e704_row5_col2\" class=\"data row5 col2\" >8871</td>\n",
       "      <td id=\"T_5e704_row5_col3\" class=\"data row5 col3\" >3.854778</td>\n",
       "      <td id=\"T_5e704_row5_col4\" class=\"data row5 col4\" >4037</td>\n",
       "      <td id=\"T_5e704_row5_col5\" class=\"data row5 col5\" >0</td>\n",
       "      <td id=\"T_5e704_row5_col6\" class=\"data row5 col6\" >230130</td>\n",
       "      <td id=\"T_5e704_row5_col7\" class=\"data row5 col7\" >5.000000</td>\n",
       "      <td id=\"T_5e704_row5_col8\" class=\"data row5 col8\" >5939.000000</td>\n",
       "      <td id=\"T_5e704_row5_col9\" class=\"data row5 col9\" >752.527382</td>\n",
       "      <td id=\"T_5e704_row5_col10\" class=\"data row5 col10\" >690.165445</td>\n",
       "      <td id=\"T_5e704_row5_col11\" class=\"data row5 col11\" >nan</td>\n",
       "      <td id=\"T_5e704_row5_col12\" class=\"data row5 col12\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e385413d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "describe_dataframe(df_train, name='Insurance Data', nrows=0, plots=False, info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "a514938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df_to_use: pd.DataFrame, impute_num_nulls: bool = True, fill_cat_nulls: bool = True) -> None:\n",
    "    df = df_to_use.copy()\n",
    "\n",
    "    # Convert date column from object to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Filter columns where the type is either 'float' or 'int' and there are missing values\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    missing_numeric_columns = [\n",
    "        col for col in numeric_cols if df[col].isnull().sum() > 0\n",
    "    ]\n",
    "\n",
    "    # Target field 'num_sold' is missing some values, we do not want to impute them\n",
    "    if 'y' in missing_numeric_columns:\n",
    "        missing_numeric_columns.remove('y')\n",
    "    \n",
    "    # Fill nulls in numeric columns with the median\n",
    "    if impute_num_nulls:       \n",
    "        for column in missing_numeric_columns:\n",
    "            mdn = df[column].median()\n",
    "            df[column] = df[column].fillna(mdn)\n",
    "\n",
    "    \n",
    "    # Get category columns\n",
    "    if fill_cat_nulls:\n",
    "        cat_cols = df.select_dtypes(include=['object', 'string','category']).columns.tolist()  \n",
    "        # Fill missing values in object columns\n",
    "        for column in cat_cols:\n",
    "            df[column] = df[column].fillna('None')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "b213027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = clean_dataframe(df_train, impute_num_nulls = True, fill_cat_nulls=False)\n",
    "df_test = clean_dataframe(df_test, impute_num_nulls = True, fill_cat_nulls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "8a79de08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zchodaniecky\\AppData\\Local\\Temp\\ipykernel_21380\\190788460.py:34: FutureWarning: Treating datetime data as categorical rather than numeric in `.describe` is deprecated and will be removed in a future version of pandas. Specify `datetime_is_numeric=True` to silence this warning and adopt the future behavior now.\n",
      "  desc = pd.DataFrame(df.describe(include='all').transpose())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0046c_row0_col2, #T_0046c_row0_col3, #T_0046c_row1_col2, #T_0046c_row1_col3, #T_0046c_row2_col2, #T_0046c_row2_col3, #T_0046c_row3_col2, #T_0046c_row3_col3, #T_0046c_row4_col2, #T_0046c_row4_col3 {\n",
       "  background-color: #fff5f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0046c_row0_col4 {\n",
       "  background-color: #00441b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0046c_row1_col4 {\n",
       "  background-color: #f6fcf4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0046c_row2_col4, #T_0046c_row3_col4, #T_0046c_row4_col4 {\n",
       "  background-color: #f7fcf5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0046c_row5_col2, #T_0046c_row5_col3 {\n",
       "  background-color: #67000d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0046c_row5_col4 {\n",
       "  background-color: #f5fbf2;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0046c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0046c_level0_col0\" class=\"col_heading level0 col0\" >column</th>\n",
       "      <th id=\"T_0046c_level0_col1\" class=\"col_heading level0 col1\" >type</th>\n",
       "      <th id=\"T_0046c_level0_col2\" class=\"col_heading level0 col2\" >missing</th>\n",
       "      <th id=\"T_0046c_level0_col3\" class=\"col_heading level0 col3\" >pct_missing</th>\n",
       "      <th id=\"T_0046c_level0_col4\" class=\"col_heading level0 col4\" >unique</th>\n",
       "      <th id=\"T_0046c_level0_col5\" class=\"col_heading level0 col5\" >duplicate</th>\n",
       "      <th id=\"T_0046c_level0_col6\" class=\"col_heading level0 col6\" >count</th>\n",
       "      <th id=\"T_0046c_level0_col7\" class=\"col_heading level0 col7\" >min</th>\n",
       "      <th id=\"T_0046c_level0_col8\" class=\"col_heading level0 col8\" >max</th>\n",
       "      <th id=\"T_0046c_level0_col9\" class=\"col_heading level0 col9\" >avg</th>\n",
       "      <th id=\"T_0046c_level0_col10\" class=\"col_heading level0 col10\" >std dev</th>\n",
       "      <th id=\"T_0046c_level0_col11\" class=\"col_heading level0 col11\" >top value</th>\n",
       "      <th id=\"T_0046c_level0_col12\" class=\"col_heading level0 col12\" >Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0046c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_0046c_row0_col0\" class=\"data row0 col0\" >id</td>\n",
       "      <td id=\"T_0046c_row0_col1\" class=\"data row0 col1\" >int64</td>\n",
       "      <td id=\"T_0046c_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_0046c_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "      <td id=\"T_0046c_row0_col4\" class=\"data row0 col4\" >230130</td>\n",
       "      <td id=\"T_0046c_row0_col5\" class=\"data row0 col5\" >0</td>\n",
       "      <td id=\"T_0046c_row0_col6\" class=\"data row0 col6\" >230130</td>\n",
       "      <td id=\"T_0046c_row0_col7\" class=\"data row0 col7\" >0.000000</td>\n",
       "      <td id=\"T_0046c_row0_col8\" class=\"data row0 col8\" >230129.000000</td>\n",
       "      <td id=\"T_0046c_row0_col9\" class=\"data row0 col9\" >115064.500000</td>\n",
       "      <td id=\"T_0046c_row0_col10\" class=\"data row0 col10\" >66432.953062</td>\n",
       "      <td id=\"T_0046c_row0_col11\" class=\"data row0 col11\" >nan</td>\n",
       "      <td id=\"T_0046c_row0_col12\" class=\"data row0 col12\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0046c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_0046c_row1_col0\" class=\"data row1 col0\" >date</td>\n",
       "      <td id=\"T_0046c_row1_col1\" class=\"data row1 col1\" >datetime64[ns]</td>\n",
       "      <td id=\"T_0046c_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_0046c_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "      <td id=\"T_0046c_row1_col4\" class=\"data row1 col4\" >2557</td>\n",
       "      <td id=\"T_0046c_row1_col5\" class=\"data row1 col5\" >0</td>\n",
       "      <td id=\"T_0046c_row1_col6\" class=\"data row1 col6\" >230130</td>\n",
       "      <td id=\"T_0046c_row1_col7\" class=\"data row1 col7\" >nan</td>\n",
       "      <td id=\"T_0046c_row1_col8\" class=\"data row1 col8\" >nan</td>\n",
       "      <td id=\"T_0046c_row1_col9\" class=\"data row1 col9\" >nan</td>\n",
       "      <td id=\"T_0046c_row1_col10\" class=\"data row1 col10\" >nan</td>\n",
       "      <td id=\"T_0046c_row1_col11\" class=\"data row1 col11\" >2010-01-01 00:00:00</td>\n",
       "      <td id=\"T_0046c_row1_col12\" class=\"data row1 col12\" >90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0046c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_0046c_row2_col0\" class=\"data row2 col0\" >country</td>\n",
       "      <td id=\"T_0046c_row2_col1\" class=\"data row2 col1\" >object</td>\n",
       "      <td id=\"T_0046c_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_0046c_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
       "      <td id=\"T_0046c_row2_col4\" class=\"data row2 col4\" >6</td>\n",
       "      <td id=\"T_0046c_row2_col5\" class=\"data row2 col5\" >0</td>\n",
       "      <td id=\"T_0046c_row2_col6\" class=\"data row2 col6\" >230130</td>\n",
       "      <td id=\"T_0046c_row2_col7\" class=\"data row2 col7\" >nan</td>\n",
       "      <td id=\"T_0046c_row2_col8\" class=\"data row2 col8\" >nan</td>\n",
       "      <td id=\"T_0046c_row2_col9\" class=\"data row2 col9\" >nan</td>\n",
       "      <td id=\"T_0046c_row2_col10\" class=\"data row2 col10\" >nan</td>\n",
       "      <td id=\"T_0046c_row2_col11\" class=\"data row2 col11\" >Canada</td>\n",
       "      <td id=\"T_0046c_row2_col12\" class=\"data row2 col12\" >38355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0046c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_0046c_row3_col0\" class=\"data row3 col0\" >store</td>\n",
       "      <td id=\"T_0046c_row3_col1\" class=\"data row3 col1\" >object</td>\n",
       "      <td id=\"T_0046c_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_0046c_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "      <td id=\"T_0046c_row3_col4\" class=\"data row3 col4\" >3</td>\n",
       "      <td id=\"T_0046c_row3_col5\" class=\"data row3 col5\" >0</td>\n",
       "      <td id=\"T_0046c_row3_col6\" class=\"data row3 col6\" >230130</td>\n",
       "      <td id=\"T_0046c_row3_col7\" class=\"data row3 col7\" >nan</td>\n",
       "      <td id=\"T_0046c_row3_col8\" class=\"data row3 col8\" >nan</td>\n",
       "      <td id=\"T_0046c_row3_col9\" class=\"data row3 col9\" >nan</td>\n",
       "      <td id=\"T_0046c_row3_col10\" class=\"data row3 col10\" >nan</td>\n",
       "      <td id=\"T_0046c_row3_col11\" class=\"data row3 col11\" >Discount Stickers</td>\n",
       "      <td id=\"T_0046c_row3_col12\" class=\"data row3 col12\" >76710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0046c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_0046c_row4_col0\" class=\"data row4 col0\" >product</td>\n",
       "      <td id=\"T_0046c_row4_col1\" class=\"data row4 col1\" >object</td>\n",
       "      <td id=\"T_0046c_row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "      <td id=\"T_0046c_row4_col3\" class=\"data row4 col3\" >0.000000</td>\n",
       "      <td id=\"T_0046c_row4_col4\" class=\"data row4 col4\" >5</td>\n",
       "      <td id=\"T_0046c_row4_col5\" class=\"data row4 col5\" >0</td>\n",
       "      <td id=\"T_0046c_row4_col6\" class=\"data row4 col6\" >230130</td>\n",
       "      <td id=\"T_0046c_row4_col7\" class=\"data row4 col7\" >nan</td>\n",
       "      <td id=\"T_0046c_row4_col8\" class=\"data row4 col8\" >nan</td>\n",
       "      <td id=\"T_0046c_row4_col9\" class=\"data row4 col9\" >nan</td>\n",
       "      <td id=\"T_0046c_row4_col10\" class=\"data row4 col10\" >nan</td>\n",
       "      <td id=\"T_0046c_row4_col11\" class=\"data row4 col11\" >Holographic Goose</td>\n",
       "      <td id=\"T_0046c_row4_col12\" class=\"data row4 col12\" >46026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0046c_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_0046c_row5_col0\" class=\"data row5 col0\" >y</td>\n",
       "      <td id=\"T_0046c_row5_col1\" class=\"data row5 col1\" >float64</td>\n",
       "      <td id=\"T_0046c_row5_col2\" class=\"data row5 col2\" >8871</td>\n",
       "      <td id=\"T_0046c_row5_col3\" class=\"data row5 col3\" >3.854778</td>\n",
       "      <td id=\"T_0046c_row5_col4\" class=\"data row5 col4\" >4037</td>\n",
       "      <td id=\"T_0046c_row5_col5\" class=\"data row5 col5\" >0</td>\n",
       "      <td id=\"T_0046c_row5_col6\" class=\"data row5 col6\" >230130</td>\n",
       "      <td id=\"T_0046c_row5_col7\" class=\"data row5 col7\" >5.000000</td>\n",
       "      <td id=\"T_0046c_row5_col8\" class=\"data row5 col8\" >5939.000000</td>\n",
       "      <td id=\"T_0046c_row5_col9\" class=\"data row5 col9\" >752.527382</td>\n",
       "      <td id=\"T_0046c_row5_col10\" class=\"data row5 col10\" >690.165445</td>\n",
       "      <td id=\"T_0046c_row5_col11\" class=\"data row5 col11\" >nan</td>\n",
       "      <td id=\"T_0046c_row5_col12\" class=\"data row5 col12\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e380f8e580>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "describe_dataframe(df_train, name='Insurance Data', nrows=0, plots=False, info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e93f87d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna(subset='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "1c23432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_holiday_name(country_code, date_obj):\n",
    "    try:\n",
    "        country_holiday = holidays.CountryHoliday(country_code, years=date_obj.year)\n",
    "        return country_holiday.get(date_obj)\n",
    "    except Exception as e:\n",
    "        print(f\"Error for country code {country_code} and date {date_obj}: {e}\")\n",
    "        return 'Invalid Holiday'\n",
    "    return country_holiday.get(date_obj)\n",
    "\n",
    "def get_country_code(country_name):\n",
    "    try:\n",
    "        country = pycountry.countries.get(name=country_name)\n",
    "        return country.alpha_2  \n",
    "    except KeyError:\n",
    "        print(f\"Unknown Country: {country_name}\")\n",
    "        return None\n",
    "\n",
    "def get_holiday_for_row(row):\n",
    "    country_code = get_country_code(row['country'])\n",
    "    if country_code is None:\n",
    "        return 'Unknown Country'     \n",
    "    try:\n",
    "        date_obj = row['date']\n",
    "    except ValueError:\n",
    "        print(f\"Invalid Date: {row['date']}\")\n",
    "        return 'Invalid Date'\n",
    "\n",
    "    return get_holiday_name(country_code, date_obj)\n",
    "\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    df['holiday'] = df.apply(get_holiday_for_row, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "35f7d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train, df_test]:\n",
    "    df['Year'] = df['date'].dt.year\n",
    "    df['Quarter'] = df['date'].dt.quarter\n",
    "    df['Month'] = df['date'].dt.month\n",
    "    df['Day'] = df['date'].dt.day\n",
    "    df['day_of_week'] = df['date'].dt.day_name()\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['Day'] / 365.0)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['Day'] / 365.0)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['Month'] / 12.0)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['Month'] / 12.0)\n",
    "    df['year_sin'] = np.sin(2 * np.pi * df['Year'] / 7.0)\n",
    "    df['year_cos'] = np.cos(2 * np.pi * df['Year'] / 7.0)\n",
    "    df['Group']=(df['Year']-2010)*48+df['Month']*4+df['Day']//7\n",
    "\n",
    "\n",
    "    df['Quarter'] = df['Quarter'].astype('str')\n",
    "    df['Month'] = df['Month'].astype('str')\n",
    "    df['day_of_week'] = df['day_of_week'].astype('str')\n",
    "    df['day_of_year'] = df['day_of_week'].astype('str')\n",
    "    df['week_of_year'] = df['week_of_year'].astype('str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "f80b5ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE 18 BASIC FEATURES ARE:\n",
      "country (categorical) with 12 unique values\n",
      "store (categorical) with 6 unique values\n",
      "product (categorical) with 10 unique values\n",
      "holiday (categorical) with 155 unique values\n",
      "Year (numerical) with 10 unique values\n",
      "Quarter (categorical) with 8 unique values\n",
      "Month (categorical) with 24 unique values\n",
      "Day (numerical) with 31 unique values\n",
      "day_of_week (categorical) with 14 unique values\n",
      "day_of_year (categorical) with 14 unique values\n",
      "week_of_year (categorical) with 105 unique values\n",
      "day_sin (numerical) with 31 unique values\n",
      "day_cos (numerical) with 31 unique values\n",
      "month_sin (numerical) with 8 unique values\n",
      "month_cos (numerical) with 8 unique values\n",
      "year_sin (numerical) with 7 unique values\n",
      "year_cos (numerical) with 4 unique values\n",
      "Group (numerical) with 481 unique values\n",
      "\n",
      "THE FOLLOWING HAVE 9 OR MORE UNIQUE VALUES: ['country', 'product', 'holiday', 'Year', 'Month', 'Day', 'day_of_week', 'day_of_year', 'week_of_year', 'day_sin', 'day_cos', 'Group']\n"
     ]
    }
   ],
   "source": [
    "RMV = ['y','id','date']\n",
    "FEATURES = [c for c in df.columns if not c in RMV]\n",
    "combined = pd.concat([df_train,df_solve],axis=0,ignore_index=True)\n",
    "\n",
    "CATS = []\n",
    "HIGH_CARDINALITY = []\n",
    "print(f\"THE {len(FEATURES)} BASIC FEATURES ARE:\")\n",
    "\n",
    "for c in FEATURES:\n",
    "    ftype = \"numerical\"\n",
    "    if combined[c].dtype==\"object\":\n",
    "        CATS.append(c)\n",
    "        combined[c] = combined[c].fillna(\"NAN\")\n",
    "        combined[c],_ = combined[c].factorize()\n",
    "        combined[c] -= combined[c].min()\n",
    "        ftype = \"categorical\"\n",
    "    if combined[c].dtype==\"int64\":\n",
    "        combined[c] = combined[c].astype(\"int32\")\n",
    "    elif combined[c].dtype==\"float64\":\n",
    "        combined[c] = combined[c].astype(\"float32\")\n",
    "\n",
    "    n = combined[c].nunique()\n",
    "    print(f\"{c} ({ftype}) with {n} unique values\")\n",
    "    if n>=9: HIGH_CARDINALITY.append(c)\n",
    "    \n",
    "df_train = combined.iloc[:len(df_train)].copy()\n",
    "df_test = combined.iloc[len(df_train):].reset_index(drop=True).copy()\n",
    "\n",
    "print(\"\\nTHE FOLLOWING HAVE 9 OR MORE UNIQUE VALUES:\", HIGH_CARDINALITY )\n",
    "\n",
    "\n",
    "df_train = df_train.drop(['date','id'], axis=1)\n",
    "df_test = df_test.drop(['date','id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1782e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57239c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_dataframe(df_train, name='Sales Data', nrows=0, plots=False, info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b962a-1958-4b6f-bcd9-73142d926e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38d996-fc5c-411f-bffb-b822e0fe1628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd12d1d-ba9a-4866-a11e-34d593d8d1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab706442-bd68-4afd-a436-cc883e19576b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69156aa2-4e9b-4dd3-b8b7-c955dac099f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ba286-de3f-4725-a98d-d7a09430ad60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e229d-3501-4cd5-88e6-f9cd5f6d2b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(24,8)})\n",
    "# ax=sns.lineplot(data=df_train,x='date',y='y',hue='product')\n",
    "# ax.axes.set_title(\"\\nBasic Time Series of Sales\\n\",fontsize=20);`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab92afe-b0b1-4bcf-899d-09a93a10e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(24,8)})\n",
    "# ax=sns.lineplot(data=df_train,x='Day of Year',y='y')\n",
    "# ax.axes.set_title(\"\\nBasic Time Series of Sales\\n\",fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f5a96-8033-41f3-a2b4-c80ebca1207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(24,8)})\n",
    "# ax=sns.lineplot(data=df_train,x='Week of Year',y='y')\n",
    "# ax.axes.set_title(\"\\nBasic Time Series of Sales\\n\",fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e519bab-dc82-4f69-9f00-e3ed80784f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(24,8)})\n",
    "# ax=sns.lineplot(data=df_train,x='Month',y='y')\n",
    "# ax.axes.set_title(\"\\nBasic Time Series of Sales\\n\",fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868ad7c8-b818-477c-a67c-cfc65738c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(24,8)})\n",
    "# ax=sns.lineplot(data=df_train,x='Year',y='y', hue='country')\n",
    "# ax.axes.set_title(\"\\nBasic Time Series of Sales\\n\",fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722c398-32e3-4858-9724-04c42f56775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(24,8)})\n",
    "# ax=sns.lineplot(data=df_train,x='Day of Year Sin',y='y')\n",
    "# ax.axes.set_title(\"\\nBasic Time Series of Sales\\n\",fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee417c-537f-4ee5-b8e6-16caac18a081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode(train, valid, test, col, target=\"y\", kfold=5, smooth=20, agg=\"mean\"):\n",
    "             \n",
    "    train['kfold'] = ((train.index) % kfold)\n",
    "    col_name = '_'.join(col)\n",
    "    train[f'TE_{agg.upper()}_' + col_name] = 0.\n",
    "    for i in range(kfold):\n",
    "        \n",
    "        df_tmp = train[train['kfold']!=i]       \n",
    "        if agg==\"mean\": mn = train[target].mean()\n",
    "        elif agg==\"median\": mn = train[target].median()\n",
    "        elif agg==\"min\": mn = train[target].min()\n",
    "        elif agg==\"max\": mn = train[target].max()\n",
    "        elif agg==\"nunique\": mn = 0\n",
    "        df_tmp = df_tmp[col + [target]].groupby(col).agg([agg, 'count']).reset_index()\n",
    "        df_tmp.columns = col + [agg, 'count']\n",
    "        if agg==\"nunique\":\n",
    "            df_tmp['TE_tmp'] = df_tmp[agg] / df_tmp['count']\n",
    "        else:\n",
    "            df_tmp['TE_tmp'] = ((df_tmp[agg]*df_tmp['count'])+(mn*smooth)) / (df_tmp['count']+smooth)\n",
    "        df_tmp_m = train[col + ['kfold', f'TE_{agg.upper()}_' + col_name]].merge(df_tmp, how='left', left_on=col, right_on=col)\n",
    "        df_tmp_m.loc[df_tmp_m['kfold']==i, f'TE_{agg.upper()}_' + col_name] = df_tmp_m.loc[df_tmp_m['kfold']==i, 'TE_tmp']\n",
    "        train[f'TE_{agg.upper()}_' + col_name] = df_tmp_m[f'TE_{agg.upper()}_' + col_name].fillna(mn).values  \n",
    "    \n",
    "    df_tmp = train[col + [target]].groupby(col).agg([agg, 'count']).reset_index()\n",
    "    if agg==\"mean\": mn = train[target].mean()\n",
    "    elif agg==\"median\": mn = train[target].median()\n",
    "    elif agg==\"min\": mn = train[target].min()\n",
    "    elif agg==\"max\": mn = train[target].max()\n",
    "    elif agg==\"nunique\": mn = 0\n",
    "    df_tmp.columns = col + [agg, 'count']\n",
    "    \n",
    "    if agg==\"nunique\":\n",
    "        df_tmp['TE_tmp'] = df_tmp[agg] / df_tmp['count']\n",
    "    else:\n",
    "        df_tmp['TE_tmp'] = ((df_tmp[agg]*df_tmp['count'])+(mn*smooth)) / (df_tmp['count']+smooth)\n",
    "        \n",
    "    valid_col_df = valid[col]\n",
    "    df_tmp_m = valid_col_df.merge(df_tmp, how='left', left_on=col, right_on=col).reset_index(drop=True)\n",
    "    valid[f'TE_{agg.upper()}_' + col_name] = df_tmp_m['TE_tmp'].fillna(mn).values\n",
    "    valid[f'TE_{agg.upper()}_' + col_name] = valid[f'TE_{agg.upper()}_' + col_name].astype(\"float32\")\n",
    "    \n",
    "    test_col_df = test[col]\n",
    "    df_tmp_m = test_col_df.merge(df_tmp, how='left', left_on=col, right_on=col).reset_index(drop=True)\n",
    "    test[f'TE_{agg.upper()}_' + col_name] = df_tmp_m['TE_tmp'].fillna(mn).values\n",
    "    test[f'TE_{agg.upper()}_' + col_name] = test[f'TE_{agg.upper()}_' + col_name].astype(\"float32\")\n",
    "\n",
    "    train = train.drop('kfold', axis=1)\n",
    "    train[f'TE_{agg.upper()}_' + col_name] = train[f'TE_{agg.upper()}_' + col_name].astype(\"float32\")\n",
    "\n",
    "    return(train, valid, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b98f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3750a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists2 = [['Annual Income', 'Health Score'], ['Credit Score', 'Health Score'], ['Customer Feedback', 'Gender', 'Marital Status', 'Occupation', 'Smoking Status', 'year'], ['Exercise Frequency', 'Health Score'], ['Health Score', 'Marital Status'], ['Education Level', 'Gender', 'Health Score'], ['Health Score', 'Occupation'], ['Age', 'Health Score'], ['Health Score', 'dow'], ['Age', 'Exercise Frequency', 'Location'], ['Health Score', 'Smoking Status', 'month'], ['Health Score', 'Location', 'Policy Type'], ['Health Score', 'Insurance Duration'], ['Health Score', 'Number of Dependents'], ['Customer Feedback', 'Exercise Frequency', 'Previous Claims', 'Property Type', 'dow'], ['Customer Feedback', 'Health Score'], ['Health Score', 'Property Type'], ['Health Score', 'day', 'seconds'], ['Health Score', 'year'], ['Age', 'Gender', 'Insurance Duration', 'year']]\n",
    "# print(f\"We have {len(lists2)} powerful combination of columns!\")\n",
    "# print(lists2)\n",
    "lists2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f7fc9-6a3d-419f-b6f3-5dcfbbce6771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb, time\n",
    "print(f\"Using XGBoost version\",xgb.__version__)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "FOLDS = 20\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(df_train))\n",
    "pred = np.zeros(len(df_test))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(df_train)):\n",
    "\n",
    "    print(\"#\"*25)\n",
    "    print(f\"### Fold {i+1}\")\n",
    "    print(\"#\"*25)\n",
    "    \n",
    "    x_train = df_train.loc[train_index,FEATURES+[\"y\"] ].copy()\n",
    "    y_train = df_train.loc[train_index,\"y\"]\n",
    "    x_valid = df_train.loc[test_index,FEATURES].copy()\n",
    "    y_valid = df_train.loc[test_index,\"y\"]\n",
    "    x_test = df_test[FEATURES].copy()\n",
    "\n",
    "    start = time.time()\n",
    "    print(f\"FEATURE ENGINEER {len(FEATURES)} COLUMNS and {len(lists2)} GROUPS: \",end=\"\")\n",
    "    for j,f in enumerate(FEATURES+lists2):\n",
    "\n",
    "        if j<len(FEATURES): c = [f]\n",
    "        else: c = f \n",
    "        print(f\"({j+1}){c}\",\", \",end=\"\")\n",
    "\n",
    "        # LOW CARDINALITY FEATURES - TARGET ENCODE MEAN AND MEDIAN\n",
    "        x_train, x_valid, x_test = target_encode(x_train, x_valid, x_test, c, smooth=20, agg=\"mean\")\n",
    "        x_train, x_valid, x_test = target_encode(x_train, x_valid, x_test, c, smooth=0, agg=\"median\")\n",
    "\n",
    "        # HIGH CARDINALITY FEATURES - TE MIN, MAX, NUNIQUE and CE\n",
    "        if (j>=len(FEATURES)) | (c[0] in HIGH_CARDINALITY):\n",
    "            x_train, x_valid, x_test = target_encode(x_train, x_valid, x_test, c, smooth=0, agg=\"min\")\n",
    "            x_train, x_valid, x_test = target_encode(x_train, x_valid, x_test, c, smooth=0, agg=\"max\")\n",
    "            x_train, x_valid, x_test = target_encode(x_train, x_valid, x_test, c, smooth=0, agg=\"nunique\")\n",
    "    \n",
    "            # COUNT ENCODING (USING COMBINED TRAIN TEST)\n",
    "            tmp = combined.groupby(c).y.count()\n",
    "            nm = f\"CE_{'_'.join(c)}\"; tmp.name = nm\n",
    "            x_train = x_train.merge(tmp, on=c, how=\"left\")\n",
    "            x_valid = x_valid.merge(tmp, on=c, how=\"left\")\n",
    "            x_test = x_test.merge(tmp, on=c, how=\"left\")\n",
    "            x_train[nm] = x_train[nm].astype(\"int32\")\n",
    "            x_valid[nm] = x_valid[nm].astype(\"int32\")\n",
    "            x_test[nm] = x_test[nm].astype(\"int32\")\n",
    "            \n",
    "    end = time.time()\n",
    "    elapsed = end-start\n",
    "    print(f\"Feature engineering took {elapsed:.1f} seconds\")\n",
    "    x_train = x_train.drop(\"y\",axis=1)\n",
    "    \n",
    "    model = XGBRegressor(\n",
    "        device=\"cuda\",\n",
    "        max_depth=8, \n",
    "        colsample_bytree=0.9, \n",
    "        subsample=0.9, \n",
    "        n_estimators=2_000, \n",
    "        learning_rate=0.01, \n",
    "        early_stopping_rounds=25,  \n",
    "        eval_metric=\"rmse\",\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        eval_set=[(x_valid, y_valid)],   \n",
    "        verbose=100\n",
    "    )\n",
    "\n",
    "    # INFER OOF\n",
    "    oof[test_index] = model.predict(x_valid)\n",
    "    # INFER TEST\n",
    "    pred += model.predict(x_test)\n",
    "\n",
    "    m = np.sqrt(np.mean( (y_valid.to_numpy() - oof[test_index])**2.0 )) \n",
    "    print(f\" => Fold {i+1} RMSLE = {m:.5f}\")\n",
    "    \n",
    "# COMPUTE AVERAGE TEST PREDS\n",
    "pred /= FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd930c7c-10c7-4080-98c8-d7097d893708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f1d3e-8409-46a2-a2f0-077d355d28c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7943187c-1a08-47f9-abf6-a2ca00d987d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####\n",
    "# # Try Bayesian Optimization\n",
    "# #### \n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from skopt import BayesSearchCV\n",
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# # Hyperparameters for BayesSearchCV tuning\n",
    "# search_spaces = {\n",
    "#     'n_estimators': (10, 200),      \n",
    "#     'max_depth': (3, 10),            \n",
    "#     'reg_alpha': (0.001, 0.2, 'log-uniform'),  \n",
    "#     'reg_lambda': (0.1, 100, 'log-uniform')     \n",
    "# }\n",
    "\n",
    "# # Leave some cores available\n",
    "# available_cores = os.cpu_count()\n",
    "# n_cores = max(1, available_cores - 3)  # Leave 3 cores free\n",
    "\n",
    "\n",
    "# xgb = XGBRegressor(random_state=SEED)\n",
    "\n",
    "# # Set up BayesSearchCV for hyperparameter tuning\n",
    "# search = BayesSearchCV(\n",
    "#     estimator=xgb,\n",
    "#     search_spaces=search_spaces,\n",
    "#     n_iter=50,          # Number of iterations for optimization\n",
    "#     cv=5,               # k-fold cross-validation\n",
    "#     verbose=1,          # Display detailed logs\n",
    "#     scoring=\"neg_root_mean_squared_error\",\n",
    "#     random_state=42     \n",
    "# )\n",
    "\n",
    "# if skip_hypertuning:\n",
    "\n",
    "#     best_params = {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 1200}\n",
    "\n",
    "# else:\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     grid_search.fit(X_train_prepared, y_log_train)\n",
    "    \n",
    "#     end_time = time.time()  \n",
    "    \n",
    "#     best_params = grid_search.best_params_\n",
    "#     best_rmse = (-grid_search.best_score_) ** 0.5\n",
    "    \n",
    "#     print(\"Best parameters:\",best_params)\n",
    "#     print(\"Best RMSE:\", best_rmse)\n",
    "\n",
    "#     # Calculate elapsed time in minutes\n",
    "#     elapsed_time_minutes = (end_time - start_time) / 60\n",
    "#     print(f\"Elapsed time: {elapsed_time_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb54ec-48cd-4b1f-b769-0075e8c108d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initialize the XGBRegressor\n",
    "# model = XGBRegressor(\n",
    "#     **best_params,\n",
    "#     random_state=SEED,        # Ensure reproducibility\n",
    "#     objective='reg:squarederror'\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(\n",
    "#     X_train_prepared, \n",
    "#     y_train, \n",
    "#     eval_set=[(X_val_prepared, y_val)],\n",
    "#     verbose=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831fd6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253c8a2-3856-4d21-a14f-df331c8504af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "#Plot feature importance\n",
    "plot_importance(model, importance_type='gain', max_num_features=min(X_train_prepared.shape[1], 25))  \n",
    "# 'weight', 'gain', 'cover' are possible importance types\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d94ded-695e-4201-9588-14be6a3ef7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_test_pred = model.predict(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db047f-5947-42a5-81f9-538d04684368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Calculate RMSE\n",
    "mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "print(\"MAPE:\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53182a9-d409-4f65-ac19-bd4e0bef8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work Dataset\n",
    "# RMSE: 1.05554 (FrequencyEncoder)\n",
    "# RMSE: 1.05597 (OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7172e43-af59-496d-9277-16b465b74651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home Dataset\n",
    "# RMSE: 1.04541 (FrequencyEncoder)\n",
    "# RMSE: 1.04536 (OneHot Encode): Submit Result = 1.0567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e77c9fb-49f6-40b9-ab84-6bf1d9f0f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final predictions\n",
    "df_solve_cleaned = clean_dataframe(df_solve, impute_num_nulls = True, fill_cat_nulls=False)\n",
    "df_solve_prepared = transform_dataframe(df_solve_cleaned)\n",
    "X_solve_prepared, X_solve_prepared_df = pipeline_transform(df_solve_prepared, pipeline, cat_encoder = cat_encoder_type)\n",
    "\n",
    "y_solve = model.predict(X_solve_prepared)\n",
    "\n",
    "df_y_solve = pd.DataFrame(y_solve, columns=['y'])\n",
    "df_final = pd.merge(df_solve_id, df_y_solve, left_index=True, right_index=True, how='inner')\n",
    "df_final.to_csv('Predictions_XGBoost.csv', index=False)\n",
    "\n",
    "df_final['y'].hist(bins=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
